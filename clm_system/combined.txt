
=== File: ./api/routes.py ===
# clm_system/api/routes.py
import logging
import os
import tempfile
from typing import Dict, Any, List, Optional

from fastapi import APIRouter, File, UploadFile, Form, HTTPException, Query
from fastapi.responses import JSONResponse
from pydantic import ValidationError

from clm_system.core.pipeline.orchestrator import PipelineService
from clm_system.core.queryEngine.search import QueryRouter
from clm_system.core.preprocessing.pdf_processor import PDFProcessor
from clm_system.schemas.schemas import ContractCreate

router = APIRouter()
logger = logging.getLogger(__name__)

# Initialize services
pipeline = PipelineService()
query_router = QueryRouter()
pdf_processor = PDFProcessor()

@router.post("/contracts/ingest", response_model=Dict[str, Any])
async def ingest_contract(contract: ContractCreate):
    """
    Ingest a contract in JSON format.
    
    Args:
        contract: Contract data in JSON format
    
    Returns:
        Processed contract information
    """
    try:
        result = await pipeline.process_document(contract.dict())
        return result
    except Exception as e:
        logger.error(f"Error ingesting contract: {e}")
        raise HTTPException(status_code=500, detail=f"Error ingesting contract: {str(e)}")

@router.post("/contracts/ingest-pdf", response_model=Dict[str, Any])
async def ingest_pdf_contract(
    file: UploadFile = File(...),
    title: Optional[str] = Form(None),
    contract_type: Optional[str] = Form(None),
    tags: Optional[str] = Form(None)
):
    """
    Ingest a contract from a PDF file.
    
    Args:
        file: PDF file
        title: Optional title override
        contract_type: Optional contract type
        tags: Optional comma-separated tags
    
    Returns:
        Processed contract information
    """
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="File must be a PDF")
    
    # Save uploaded file temporarily
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pdf')
    try:
        # Write file content
        content = await file.read()
        temp_file.write(content)
        temp_file.close()
        
        # Process PDF
        contract_data = await pdf_processor.process_pdf(temp_file.name)
        
        # Override with provided form data if any
        if title:
            contract_data["title"] = title
        if contract_type:
            contract_data["metadata"]["contract_type"] = contract_type
        if tags:
            contract_data["metadata"]["tags"] = [tag.strip() for tag in tags.split(',')]
        
        # Validate contract structure
        try:
            contract_obj = ContractCreate.parse_obj(contract_data)
        except ValidationError as ve:
            logger.error(f"Invalid contract structure: {ve}")
            raise HTTPException(status_code=400, detail=f"Invalid contract structure: {ve}")
        
        # Process through pipeline
        result = await pipeline.process_document(contract.dict())
        return result
        
    except Exception as e:
        logger.error(f"Error processing PDF contract: {e}")
        raise HTTPException(status_code=500, detail=f"Error processing PDF: {str(e)}")
    finally:
        # Clean up temporary file
        if os.path.exists(temp_file.name):
            os.unlink(temp_file.name)

@router.get("/contracts/{contract_id}", response_model=Dict[str, Any])
async def get_contract(contract_id: str):
    """
    Get contract by ID.
    
    Args:
        contract_id: Contract ID
    
    Returns:
        Contract data
    """
    try:
        # Assuming pipeline or a repository has this method
        contract = await pipeline.get_contract(contract_id)
        if not contract:
            raise HTTPException(status_code=404, detail="Contract not found")
        return contract
    except Exception as e:
        logger.error(f"Error getting contract: {e}")
        raise HTTPException(status_code=500, detail=f"Error retrieving contract: {str(e)}")

@router.get("/contracts", response_model=List[Dict[str, Any]])
async def list_contracts(
    limit: int = Query(100, ge=1, le=1000),
    skip: int = Query(0, ge=0),
    contract_type: Optional[str] = None,
    status: Optional[str] = None
):
    """
    List contracts with optional filtering.
    
    Args:
        limit: Maximum number of contracts to return
        skip: Number of contracts to skip
        contract_type: Filter by contract type
        status: Filter by status
    
    Returns:
        List of contracts
    """
    try:
        # Build filters
        filters = {}
        if contract_type:
            filters["metadata.contract_type"] = contract_type
        if status:
            filters["metadata.status"] = status
            
        # Assuming pipeline or a repository has this method
        contracts = await pipeline.get_contracts(filters, limit, skip)
        return contracts
    except Exception as e:
        logger.error(f"Error listing contracts: {e}")
        raise HTTPException(status_code=500, detail=f"Error listing contracts: {str(e)}")

@router.post("/search", response_model=Dict[str, Any])
async def search_contracts(
    query: str,
    filters: Optional[Dict[str, Any]] = None,
    top_k: int = Query(5, ge=1, le=100)
):
    """
    Search contracts using the query router.
    
    Args:
        query: Search query
        filters: Optional filters
        top_k: Number of results to return
    
    Returns:
        Search results
    """
    try:
        results = await query_router.route_query(query, filters, top_k)
        return results
    except Exception as e:
        logger.error(f"Error performing search: {e}")
        raise HTTPException(status_code=500, detail=f"Error performing search: {str(e)}")

=== End of ./api/routes.py ===


=== File: ./api/__init__.py ===


=== End of ./api/__init__.py ===


=== File: ./cli.py ===
#!/usr/bin/env python
"""
Command-line interface for CLM Smart Search System.
Allows users to ingest contracts, search, and manage the system.
"""
import asyncio
import json
import logging
import os
import functools

import click
from dotenv import load_dotenv
from pydantic import ValidationError

from clm_system.schemas.schemas import ContractCreate
from clm_system.core.pipeline.orchestrator import PipelineService
from clm_system.core.query_engine.search import QueryRouter
from clm_system.core.utils.embeddings import get_embedding_model, compute_embedding
from clm_system.core.pipeline.preprocessing.pdf_processor import PDFProcessor
# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=getattr(logging, os.getenv("LOG_LEVEL", "INFO")),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger("clm-cli")

# Initialize services
pipeline = PipelineService()      # no args, uses its internal defaults
query_router = QueryRouter()

def async_command(func):
    """Decorator to run async commands."""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        return asyncio.run(func(*args, **kwargs))
    return wrapper

@click.group()
def cli():
    """CLM Smart Search System CLI"""
    pass

@cli.command()
@click.argument('file_path', type=click.Path(exists=True))
@async_command
async def ingest(file_path):
    """Ingest a contract JSON file into the system."""
    # 1) Load raw JSON
    try:
        raw = json.load(open(file_path, 'r'))
    except Exception as e:
        click.echo(f"Failed to read file {file_path}: {e}", err=True)
        return

    # 2) Validate & coerce with Pydantic
    try:
        contract_obj = ContractCreate.parse_obj(raw)
    except ValidationError as ve:
        click.echo("Invalid contract JSON:", err=True)
        click.echo(ve.json(), err=True)
        return

    # 3) Run through your pipeline
    try:
        result = await pipeline.process_document(contract_obj.dict())
    except Exception as e:
        click.echo(f"Error ingesting contract: {e}", err=True)
        return

    # 4) Success output
    click.echo(f"Contract ingested successfully: {result['id']}")
    click.echo(f"Title: {result['title']}")
    click.echo(f"Clauses: {result['clauses_count']}")
    click.echo(f"Status: {result['status']}")

@cli.command()
@click.argument('query')
@click.option('--filters', '-f', help='JSON string with filters')
@click.option('--top-k', '-k', type=int, default=5, help='Number of results to return')
@async_command
async def search(query, filters=None, top_k=5):
    """Search for contracts using the query router."""
    # Parse filters JSON if provided
    try:
        filter_dict = json.loads(filters) if filters else None
    except Exception as e:
        click.echo(f"Invalid filters JSON: {e}", err=True)
        return

    # Perform search
    try:
        results = await query_router.route_query(query, filter_dict, top_k)
    except Exception as e:
        click.echo(f"Error performing search: {e}", err=True)
        return

    # Display results
    click.echo(f"Query: {results['query']}")
    click.echo(f"Query type: {results['metadata']['query_type']}")
    click.echo(f"Total results: {results['total_results']}")
    click.echo(f"Execution time: {results['execution_time_ms']:.2f}ms")
    click.echo("\nResults:")
    for i, r in enumerate(results['results'], 1):
        click.echo(f"\n--- Result {i} ---")
        click.echo(f"Contract: {r['contract_title']} (ID: {r['contract_id']})")
        click.echo(f"Clause: {r.get('clause_title', r['clause_type'])}")
        click.echo(f"Relevance: {r['relevance_score']:.4f}")
        click.echo(f"Content: {r['content'][:100]}...")

@cli.command()
@click.option('--model-name', '-m', help='Override the default embedding model')
def test_embedding(model_name=None):
    """Test the embedding model with a sample text."""
    try:
        if model_name:
            os.environ["EMBEDDING_MODEL"] = model_name
            click.echo(f"Using model: {model_name}")
        else:
            click.echo(f"Using default model: {os.getenv('EMBEDDING_MODEL')}")

        model = get_embedding_model()
        sample_text = "This is a test sentence to verify the embedding model is working correctly."
        embedding = compute_embedding(sample_text, model)

        click.echo(f"Successfully computed embedding with dimensions: {len(embedding)}")
        click.echo(f"First 5 values: {embedding[:5]}")
    except Exception as e:
        click.echo(f"Error testing embedding model: {e}", err=True)


@cli.command()
@click.argument('pdf_path', type=click.Path(exists=True))
@click.option('--title', '-t', help='Optional title override')
@click.option('--contract-type', '-c', help='Optional contract type')
@click.option('--tags', help='Optional comma-separated tags')
@async_command
async def ingest_pdf(pdf_path, title=None, contract_type=None, tags=None):
    """Ingest a contract from a PDF file."""
    try:
        if not pdf_path.lower().endswith('.pdf'):
            click.echo("File must be a PDF", err=True)
            return
            
        # Initialize PDF processor
        pdf_processor = PDFProcessor()
        
        # Process PDF
        click.echo(f"Processing PDF: {pdf_path}")
        contract_data = await pdf_processor.process_pdf(pdf_path)
        
        # Override with provided options if any
        if title:
            contract_data["title"] = title
        if contract_type:
            contract_data["metadata"]["contract_type"] = contract_type
        if tags:
            contract_data["metadata"]["tags"] = [tag.strip() for tag in tags.split(',')]
        
        # Validate contract structure
        try:
            contract_obj = ContractCreate.parse_obj(contract_data)
        except ValidationError as ve:
            click.echo("Invalid contract structure:", err=True)
            click.echo(ve.json(), err=True)
            return
        
        # Process through pipeline
        result = await pipeline.process_document(contract_obj.dict())
        
        # Success output
        click.echo(f"PDF contract ingested successfully: {result['id']}")
        click.echo(f"Title: {result['title']}")
        click.echo(f"Clauses: {result['clauses_count']}")
        click.echo(f"Status: {result['status']}")
        
    except Exception as e:
        click.echo(f"Error processing PDF contract: {e}", err=True)

if __name__ == "__main__":
    cli()


=== End of ./cli.py ===


=== File: ./config.py ===
# File: clm_system/config.py
import os
from functools import lru_cache
from typing import Optional

from pydantic import BaseSettings, Field
from dotenv import load_dotenv
load_dotenv()

class Settings(BaseSettings):
    
    openai_api_key: str = os.getenv("OPENAI_API_KEY", "")
    embedding_model: str = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
    chunk_size: int = int(os.getenv("CHUNK_SIZE", "512"))
    chunk_overlap: int = int(os.getenv("CHUNK_OVERLAP", "128"))
    default_top_k: int = int(os.getenv("DEFAULT_TOP_K", "5"))
    classifier_cache_ttl: int = int(os.getenv("CLASSIFIER_CACHE_TTL", "3600"))
    
    # MongoDB settings
    mongodb_uri: str = Field(..., env="MONGODB_URI")
    mongodb_database: str = Field("clm_db", env="MONGODB_DATABASE")
    
    # Elasticsearch settings
    elasticsearch_uri: str = Field(..., env="ELASTICSEARCH_URI")
    
    # Qdrant settings
    qdrant_uri: str = Field(..., env="QDRANT_URI")
    
    # API settings
    api_host: str = Field("0.0.0.0", env="API_HOST")
    api_port: int = Field(8000, env="API_PORT")
    
    # Embedding model
    embedding_model: str = Field(
        "sentence-transformers/all-MiniLM-L6-v2", env="EMBEDDING_MODEL"
    )
    
    # Default chunk size for text splitting
    chunk_size: int = Field(500, env="CHUNK_SIZE")
    chunk_overlap: int = Field(50, env="CHUNK_OVERLAP")
    
    # Vector settings
    vector_dimension: int = Field(384, env="VECTOR_DIMENSION")  # Default for MiniLM-L6
    
    # Search settings
    default_top_k: int = Field(5, env="DEFAULT_TOP_K")
    
    class Config:
        env_file = ".env"
        case_sensitive = False
    

@lru_cache()
def get_settings() -> Settings:
    return Settings()


settings = get_settings()



=== End of ./config.py ===


=== File: ./core/database/elasticsearch_client.py ===

# File: clm_system/core/database/elasticsearch_client.py
import logging
from typing import Dict, List, Any, Optional

from elasticsearch import AsyncElasticsearch, NotFoundError

from clm_system.config import settings

logger = logging.getLogger(__name__)

class ElasticsearchClient:
    """Client for interacting with Elasticsearch."""
    
    def __init__(self):
        self.client = AsyncElasticsearch(settings.elasticsearch_uri)
        self.index_name = "contracts"
    
    async def ensure_index(self):
        """Ensures the contracts index exists with proper mappings."""
        try:
            # Check if index exists
            exists = await self.client.indices.exists(index=self.index_name)
            if not exists:
                # Create index with mappings
                mappings = {
                    "mappings": {
                        "properties": {
                            "id": {"type": "keyword"},
                            "title": {"type": "text", "analyzer": "standard"},
                            "created_at": {"type": "date"},
                            "updated_at": {"type": "date"},
                            "metadata": {
                                "properties": {
                                    "contract_type": {"type": "keyword"},
                                    "effective_date": {"type": "date"},
                                    "expiration_date": {"type": "date"},
                                    "parties": {
                                        "type": "nested",
                                        "properties": {
                                            "name": {"type": "text", "analyzer": "standard"},
                                            "id": {"type": "keyword"}
                                        }
                                    },
                                    "status": {"type": "keyword"},
                                    "tags": {"type": "keyword"}
                                }
                            },
                            "clauses": {
                                "type": "nested",
                                "properties": {
                                    "id": {"type": "keyword"},
                                    "title": {"type": "text", "analyzer": "standard"},
                                    "type": {"type": "keyword"},
                                    "text": {"type": "text", "analyzer": "standard"},
                                    "position": {"type": "integer"},
                                    "metadata": {"type": "object"}
                                }
                            }
                        }
                    }
                }
                await self.client.indices.create(index=self.index_name, body=mappings)
                logger.info(f"Created Elasticsearch index {self.index_name}")
        except Exception as e:
            logger.error(f"Error ensuring Elasticsearch index: {str(e)}")
            raise
    
    async def index_contract(self, contract: Dict[str, Any]) -> Dict[str, Any]:
        """
        Indexes a contract in Elasticsearch.
        
        Args:
            contract: Contract data
            
        Returns:
            Elasticsearch response
        """
        try:
            # Ensure index exists
            await self.ensure_index()
            
            # Index document
            response = await self.client.index(
                index=self.index_name,
                id=contract["id"],
                document=contract,
                refresh=True  # Make document immediately searchable
            )
            return response
        except Exception as e:
            logger.error(f"Elasticsearch index error: {str(e)}")
            raise
    
    async def search(
        self, query: str, filters: Optional[Dict[str, Any]] = None, top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        Searches for contracts in Elasticsearch.
        
        Args:
            query: Search query
            filters: Query filters
            top_k: Maximum number of results to return
            
        Returns:
            List of search results
        """
        try:
            await self.ensure_index()
            
            search_query = {
                "query": {
                    "bool": {
                        "must": [
                            {
                                "nested": {
                                    "path": "clauses",
                                    "query": {
                                        "multi_match": {
                                            "query": query,
                                            "fields": ["clauses.title^1.5", "clauses.text"]
                                        }
                                    },
                                    "inner_hits": {
                                        "highlight": {
                                            "fields": {
                                                "clauses.text": {"fragment_size": 150}
                                            }
                                        },
                                        "_source": True,
                                        "size": 3  # Maximum number of clause matches per contract
                                    }
                                }
                            },
                            {
                                "multi_match": {
                                    "query": query,
                                    "fields": ["title^2"]
                                }
                            }
                        ]
                    }
                },
                "size": top_k
            }

            # Add filters if provided
            if filters:
                filter_clauses = []
                for field, value in filters.items():
                    if field.startswith("metadata."):
                        filter_clauses.append({"term": {field: value}})
                    elif field.startswith("clauses."):
                        filter_clauses.append({
                            "nested": {
                                "path": "clauses",
                                "query": {"term": {field: value}}
                            }
                        })
                    else:
                        filter_clauses.append({"term": {field: value}})
                
                if filter_clauses:
                    search_query["query"]["bool"]["filter"] = filter_clauses

            response = await self.client.search(
                index=self.index_name,
                body=search_query
            )

            results = []
            for hit in response["hits"]["hits"]:
                source = hit["_source"]
                inner_hits = hit.get("inner_hits", {}).get("clauses", {}).get("hits", {}).get("hits", [])
                
                for clause_hit in inner_hits:
                    clause_source = clause_hit["_source"]
                    highlights = clause_hit.get("highlight", {}).get("clauses.text", [])
                    highlight_text = highlights[0] if highlights else clause_source.get("text", "")[:150]
                    
                    results.append({
                        "clause_id": clause_source["id"],
                        "contract_id": source["id"],
                        "contract_title": source["title"],
                        "clause_type": clause_source["type"],
                        "clause_title": clause_source.get("title"),
                        "content": highlight_text,
                        "relevance_score": hit["_score"] * clause_hit["_score"],
                        "metadata": {
                            **source.get("metadata", {}),
                            **clause_source.get("metadata", {})
                        }
                    })
            
            return sorted(results, key=lambda x: -x["relevance_score"])[:top_k]
        except Exception as e:
            logger.error(f"Elasticsearch search error: {str(e)}")
            raise
 

=== End of ./core/database/elasticsearch_client.py ===


=== File: ./core/database/mongodb_client.py ===

# File: clm_system/core/database/mongodb_client.py
import logging
from typing import Dict, List, Any, Optional

from pymongo import MongoClient
from pymongo.errors import PyMongoError

from clm_system.config import settings

logger = logging.getLogger(__name__)

class MongoDBClient:
    """Client for interacting with MongoDB."""
    
    def __init__(self):
        self.client = AsyncIOMotorClient(settings.mongodb_uri)
        self.db = self.client[settings.mongodb_database]
        self.contracts_collection = self.db.contracts
    
    async def insert_contract(self, contract: Dict[str, Any]) -> str:
        """
        Inserts a contract into the MongoDB collection.
        
        Args:
            contract: Contract data
            
        Returns:
            ID of the inserted contract
        """
        try:
            # Convert datetime to string for MongoDB
            contract = contract.copy()
            if "created_at" in contract:
                contract["created_at"] = contract["created_at"].isoformat()
            if "updated_at" in contract:
                contract["updated_at"] = contract["updated_at"].isoformat()
                
            # Use async insert
            result = await self.contracts_collection.insert_one(contract)
            return str(result.inserted_id)
        except PyMongoError as e:
            logger.error(f"MongoDB insert error: {str(e)}")
            raise
        
    async def get_contract(self, contract_id: str) -> Optional[Dict[str, Any]]:
        """
        Retrieves a contract by ID.
        
        Args:
            contract_id: ID of the contract to retrieve
            
        Returns:
            Contract data or None if not found
        """
        try:
            contract = self.contracts_collection.find_one({"id": contract_id})
            return contract
        except PyMongoError as e:
            logger.error(f"MongoDB get error: {str(e)}")
            raise
    
    async def get_contracts(
        self, filters: Optional[Dict[str, Any]] = None, limit: int = 100, skip: int = 0
    ) -> List[Dict[str, Any]]:
        """
        Retrieves contracts matching the given filters.
        
        Args:
            filters: Query filters
            limit: Maximum number of contracts to return
            skip: Number of contracts to skip
            
        Returns:
            List of contracts
        """
        try:
            query = filters or {}
            contracts = list(
                self.contracts_collection
                .find(query)
                .limit(limit)
                .skip(skip)
            )
            return contracts
        except PyMongoError as e:
            logger.error(f"MongoDB get_contracts error: {str(e)}")
            raise


=== End of ./core/database/mongodb_client.py ===


=== File: ./core/database/qdrant_client.py ===

# File: clm_system/core/database/qdrant_client.py
import logging
from typing import Dict, List, Any, Optional

from qdrant_client import QdrantClient as QdrantClientLib, models

from clm_system.config import settings

logger = logging.getLogger(__name__)

class  QdrantClient:
    """Client for interacting with Qdrant vector database."""
    
    def __init__(self):
        self.client = QdrantClientLib(url=settings.qdrant_uri)  # You might want to differentiate here if desired
        self.collection_name = "contract_clauses"
        self.vector_size = settings.vector_dimension
    
    async def ensure_collection(self):
        """Ensures the vector collection exists."""
        try:
            # Check if collection exists
            collections = self.client.get_collections().collections
            collection_names = [c.name for c in collections]
            
            if self.collection_name not in collection_names:
                # Create collection
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=self.vector_size,
                        distance=models.Distance.COSINE
                    )
                )
                
                # Create payload indexes for fast filtering
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name="contract_id",
                    field_schema=models.PayloadSchemaType.KEYWORD
                )
                self.client.create_payload_index(
                    collection_name=self.collection_name,
                    field_name="clause_type",
                    field_schema=models.PayloadSchemaType.KEYWORD
                )
                
                logger.info(f"Created Qdrant collection {self.collection_name}")
        except Exception as e:
            logger.error(f"Error ensuring Qdrant collection: {str(e)}")
            raise

    
    async def store_embedding(
    self,
    contract_id: str,
    contract_title: str,
    clause_id: str,
    clause_type: str,
    content: str,
    embedding: List[float],
    clause_title: Optional[str] = None,
    metadata: Optional[Dict[str, Any]] = None
) -> str:
        """
        Stores a clause embedding in Qdrant.
        """
        try:
            # Ensure collection exists
            await self.ensure_collection()
            
            # Build payload
            payload = {
                "contract_id": contract_id,
                "contract_title": contract_title,
                "clause_id": clause_id,  # Keep original ID in payload
                "clause_type": clause_type,
                "content": content
            }
            
            if clause_title:
                payload["clause_title"] = clause_title
                
            if metadata:
                payload["metadata"] = metadata
            
            # Convert clause_id to UUID format if it's not already a UUID
            import uuid
            point_id = clause_id
            try:
                # Check if it's already a UUID
                uuid.UUID(clause_id)
            except ValueError:
                # If not a UUID, generate a new one based on the clause_id
                point_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, clause_id))
            
            # Store point
            self.client.upsert(
                collection_name=self.collection_name,
                points=[
                    models.PointStruct(
                        id=point_id,  # Use the UUID
                        vector=embedding,
                        payload=payload
                    )
                ]
            )
            
            return clause_id
        except Exception as e:
            logger.error(f"Qdrant store error: {str(e)}")
            raise
    
    async def search(
        self, 
        embedding: List[float],
        filters: Optional[Dict[str, Any]] = None,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        Searches for similar clauses in Qdrant.
        
        Args:
            embedding: Query vector embedding
            filters: Optional filters
            top_k: Maximum number of results to return
            
        Returns:
            List of search results
        """
        try:
            # Ensure collection exists
            await self.ensure_collection()
            
            # Build filter
            filter_query = None
            if filters:
                conditions = []
                for field, value in filters.items():
                    if field == "contract_id":
                        conditions.append(
                            models.FieldCondition(
                                key="contract_id",
                                match=models.MatchValue(value=value)
                            )
                        )
                    elif field == "clause_type":
                        conditions.append(
                            models.FieldCondition(
                                key="clause_type",
                                match=models.MatchValue(value=value)
                            )
                        )
                
                if conditions:
                    filter_query = models.Filter(
                        must=conditions
                    )
            
            # Execute search
            search_results = self.client.search(
                collection_name=self.collection_name,
                query_vector=embedding,
                query_filter=filter_query,
                limit=top_k
            )
            
            # Process results
            results = []
            for hit in search_results:
                results.append({
                    "clause_id": hit.payload["clause_id"],
                    "contract_id": hit.payload["contract_id"],
                    "contract_title": hit.payload["contract_title"],
                    "clause_type": hit.payload["clause_type"],
                    "clause_title": hit.payload.get("clause_title"),
                    "content": hit.payload["content"],
                    "relevance_score": hit.score,
                    "metadata": hit.payload.get("metadata", {})
                })
            
            return results
        except Exception as e:
            logger.error(f"Qdrant search error: {str(e)}")
            raise


=== End of ./core/database/qdrant_client.py ===


=== File: ./core/database/__init__.py ===


=== End of ./core/database/__init__.py ===


=== File: ./core/pipeline/base.py ===
# clm_system/core/pipeline/base.py
from typing import Any, Dict

class IngestorABC:
    def ingest(self, raw: Any) -> Dict[str, Any]:
        """Normalize raw input into our contract dict."""
        raise NotImplementedError

class CleanerABC:
    def clean(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Apply PII scrubbing, standardization, etc."""
        raise NotImplementedError

class ChunkerABC:
    def chunk(self, text: str) -> list[str]:
        """Split long text into embedding‑ready chunks."""
        raise NotImplementedError


=== End of ./core/pipeline/base.py ===


=== File: ./core/pipeline/chunking/base.py ===
# file: core/pipeline/chunking/base.py 
from abc import ABC, abstractmethod

class ChunkerABC(ABC):
    @abstractmethod
    def chunk(self, raw: dict) -> dict:
        pass


=== End of ./core/pipeline/chunking/base.py ===


=== File: ./core/pipeline/chunking/contract.py ===
# clm_system/core/pipeline/chunking/contract.py
from typing import List
import spacy
from .base import ChunkerABC
from clm_system.config import settings

class ContractChunker(ChunkerABC):
    def __init__(self):
        # Create blank English pipeline with just the sentencizer
        self.nlp = English()
        self.nlp.add_pipe("sentencizer")
        logger.info("SpaCy pipeline components: %s", self.nlp.pipe_names)

    def chunk(self, text: str) -> List[str]:
        doc = self.nlp(text)
        sentences = [sent.text.strip() for sent in doc.sents]
        
        size = settings.chunk_size
        overlap = settings.chunk_overlap
        chunks = []
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            sentence_tokens = sentence.split()
            sentence_len = len(sentence_tokens)
            
            if sentence_len > size:
                # Handle very long sentences
                for i in range(0, sentence_len, size - overlap):
                    chunk_tokens = sentence_tokens[i:min(i + size, sentence_len)]
                    chunks.append(" ".join(chunk_tokens))
            else:
                if current_length + sentence_len > size:
                    # Finalize current chunk
                    chunks.append(" ".join(current_chunk))
                    # Start new chunk with overlap tokens from previous chunk
                    overlap_tokens = min(overlap, len(current_chunk))
                    current_chunk = current_chunk[-overlap_tokens:] if overlap_tokens > 0 else []
                    current_length = len(current_chunk)
                
                current_chunk.extend(sentence_tokens)
                current_length += sentence_len

        if current_chunk:
            chunks.append(" ".join(current_chunk))

        return chunks

=== End of ./core/pipeline/chunking/contract.py ===


=== File: ./core/pipeline/chunking/deal.py ===
# clm_system/core/pipeline/chunking/deal.py
from typing import List
import re
from .base import ChunkerABC
from clm_system.config import settings

class DealChunker(ChunkerABC):
    """Chunker for oil industry deal documents."""
    
    def chunk(self, text: str) -> List[str]:
        """
        Split deal text into semantic chunks for embedding.
        
        Args:
            text: Input text to chunk
            
        Returns:
            List of text chunks
        """
        if not text:
            return []
            
        # Get configuration
        size = settings.chunk_size
        overlap = settings.chunk_overlap
        
        # Split on section boundaries first
        sections = self._split_sections(text)
        
        # Process each section
        chunks = []
        for section in sections:
            # If section is already small enough, keep as is
            if len(section.split()) <= size:
                chunks.append(section)
                continue
                
            # Otherwise, chunk using sliding window
            tokens = section.split()
            for i in range(0, len(tokens), size - overlap):
                chunk_tokens = tokens[i:min(i + size, len(tokens))]
                if chunk_tokens:
                    chunks.append(" ".join(chunk_tokens))
        
        # Ensure each chunk is non-empty and deduplicate
        return [chunk for chunk in chunks if chunk.strip()]
    
    def _split_sections(self, text: str) -> List[str]:
        """Split text into logical sections based on headers/markers."""
        # Common section markers in oil & gas deals
        section_markers = [
            r'DEAL SUMMARY',
            r'PRICING TERMS',
            r'VOLUME DETAILS',
            r'DELIVERY TERMS',
            r'QUALITY SPECIFICATIONS',
            r'PAYMENT TERMS',
            r'SPECIAL PROVISIONS',
            r'FORCE MAJEURE',
            r'REGULATORY COMPLIANCE',
        ]
        
        # Create a regex pattern matching any of the section markers
        pattern = '|'.join(f'({marker})' for marker in section_markers)
        
        # Find all matches
        matches = list(re.finditer(pattern, text, re.IGNORECASE))
        
        if not matches:
            # No section markers found, return whole text
            return [text]
        
        # Extract sections using the positions of the matches
        sections = []
        
        # First section (from start to first match)
        if matches[0].start() > 0:
            sections.append(text[:matches[0].start()].strip())
        
        # Middle sections
        for i in range(len(matches)):
            start = matches[i].start()
            end = matches[i+1].start() if i < len(matches) - 1 else len(text)
            section_text = text[start:end].strip()
            if section_text:
                sections.append(section_text)
        
        return sections

=== End of ./core/pipeline/chunking/deal.py ===


=== File: ./core/pipeline/chunking/email.py ===
# clm_system/core/pipeline/chunking/email.py
from typing import List
import re
from .base import ChunkerABC
from clm_system.config import settings

class EmailChunker(ChunkerABC):
    """Chunker for email documents."""
    
    def chunk(self, text: str) -> List[str]:
        """
        Split email text into semantic chunks for embedding.
        
        Args:
            text: Input text to chunk
            
        Returns:
            List of text chunks
        """
        if not text:
            return []
            
        # Get configuration
        size = settings.chunk_size
        overlap = settings.chunk_overlap
        
        # First identify logical parts (greeting, body, signature, etc.)
        parts = self._split_email_parts(text)
        
        # Process each part
        chunks = []
        for part in parts:
            # Short parts can be kept as-is
            if len(part.split()) <= size:
                chunks.append(part)
                continue
                
            # For longer parts, chunk with sliding window
            tokens = part.split()
            for i in range(0, len(tokens), size - overlap):
                chunk_tokens = tokens[i:min(i + size, len(tokens))]
                if chunk_tokens:  # Ensure non-empty
                    chunks.append(" ".join(chunk_tokens))
        
        # Ensure each chunk is non-empty and deduplicate
        return [chunk for chunk in chunks if chunk.strip()]
    
    def _split_email_parts(self, text: str) -> List[str]:
        """Split email into logical parts (greeting, body, signature, etc.)."""
        # Try to identify parts based on common patterns
        
        # Check for greeting
        greeting_patterns = [
            r'^(Dear\s+[^,\n]+[,\n])',
            r'^(Hi\s+[^,\n]+[,\n])',
            r'^(Hello\s+[^,\n]+[,\n])',
            r'^(Good\s+(morning|afternoon|evening)[^,\n]*[,\n])'
        ]
        
        # Check for signature
        signature_patterns = [
            r'(Best\s+regards,?\s*\n+.+)$',
            r'(Regards,?\s*\n+.+)$',
            r'(Thanks,?\s*\n+.+)$',
            r'(Thank\s+you,?\s*\n+.+)$',
            r'(Sincerely,?\s*\n+.+)$',
            r'(--\s*\n+.+)$'
        ]
        
        # Initialize parts
        parts = []
        remaining_text = text
        
        # Extract greeting
        for pattern in greeting_patterns:
            match = re.search(pattern, remaining_text, re.IGNORECASE | re.MULTILINE)
            if match:
                greeting = match.group(1).strip()
                if greeting:
                    parts.append(greeting)
                remaining_text = remaining_text[match.end():].strip()
                break
        
        # Extract signature
        signature = None
        for pattern in signature_patterns:
            match = re.search(pattern, remaining_text, re.IGNORECASE | re.MULTILINE | re.DOTALL)
            if match:
                signature = match.group(1).strip()
                remaining_text = remaining_text[:match.start()].strip()
                break
        
        # Process the body (remaining text)
        if remaining_text:
            # Try to split by paragraphs
            paragraphs = re.split(r'\n{2,}', remaining_text)
            
            # Add each significant paragraph as a separate part
            for paragraph in paragraphs:
                cleaned = paragraph.strip()
                if cleaned and len(cleaned) > 10:  # Avoid tiny fragments
                    parts.append(cleaned)
        
        # Add signature at the end if found
        if signature:
            parts.append(signature)
        
        # If no parts were identified, return original text
        if not parts:
            return [text]
            
        return parts

=== End of ./core/pipeline/chunking/email.py ===


=== File: ./core/pipeline/cleaning/base.py ===
# file: core/pipeline/cleaning/base.py 
from abc import ABC, abstractmethod

class CleanerABC(ABC):
    @abstractmethod
    def clean(self, raw: dict) -> dict:
        pass


=== End of ./core/pipeline/cleaning/base.py ===


=== File: ./core/pipeline/cleaning/contract.py ===
# clm_system/core/pipeline/cleaning/contract.py
from .base import CleanerABC

class ContractCleaner(CleanerABC):
    def clean(self, data: dict) -> dict:
        # e.g. strip whitespace, normalize dates, remove PII
        # if already clean, return as‑is
        return data


=== End of ./core/pipeline/cleaning/contract.py ===


=== File: ./core/pipeline/cleaning/deal.py ===
# clm_system/core/pipeline/cleaning/deal.py
import re
from .base import CleanerABC

class DealCleaner(CleanerABC):
    def clean(self, data: dict) -> dict:
        """
        Clean and normalize oil industry deal data.
        
        Args:
            data: Raw deal data
            
        Returns:
            Cleaned deal data
        """
        cleaned = data.copy()
        
        # Normalize deal types (lowercase, replace spaces with underscores)
        if "metadata" in cleaned and "deal_type" in cleaned["metadata"]:
            cleaned["metadata"]["deal_type"] = cleaned["metadata"]["deal_type"].lower().replace(" ", "_")
        
        # Standardize location formatting if present
        if "metadata" in cleaned and "location" in cleaned["metadata"]:
            cleaned["metadata"]["location"] = self._normalize_location(cleaned["metadata"]["location"])
            
        # Clean monetary values
        for field in ["price_per_unit", "total_value"]:
            if "metadata" in cleaned and field in cleaned["metadata"]:
                cleaned["metadata"][field] = self._normalize_monetary_value(cleaned["metadata"][field])
        
        # Clean volume 
        if "metadata" in cleaned and "volume" in cleaned["metadata"]:
            cleaned["metadata"]["volume"] = self._normalize_volume(cleaned["metadata"]["volume"])
                
        # Clean clauses text
        if "clauses" in cleaned:
            for clause in cleaned["clauses"]:
                if "text" in clause:
                    clause["text"] = self._clean_text(clause["text"])
        
        return cleaned
    
    def _normalize_location(self, location: str) -> str:
        """Normalize location strings."""
        if not location or not isinstance(location, str):
            return location
            
        # Convert to title case
        location = location.title()
        
        # Handle common abbreviations
        abbreviations = {
            " Usa": " USA",
            " Uk": " UK",
            " Uae": " UAE",
        }
        for abbr, replacement in abbreviations.items():
            location = location.replace(abbr, replacement)
            
        return location
    
    def _normalize_monetary_value(self, value):
        """Normalize monetary values to numeric format."""
        if isinstance(value, (int, float)):
            return value
            
        if not value or not isinstance(value, str):
            return value
            
        # Remove currency symbols and commas
        value = re.sub(r'[$£€,]', '', value)
        
        # Extract numeric part
        match = re.search(r'([\d.]+)', value)
        if match:
            try:
                return float(match.group(1))
            except ValueError:
                pass
                
        return value
    
    def _normalize_volume(self, volume):
        """Normalize volume values."""
        if isinstance(volume, (int, float)):
            return volume
            
        if not volume or not isinstance(volume, str):
            return volume
            
        # Remove commas, standardize units
        volume = re.sub(r'[,]', '', volume)
        
        # Match number and unit
        match = re.search(r'([\d.]+)\s*([a-zA-Z]+)', volume)
        if match:
            try:
                value = float(match.group(1))
                unit = match.group(2).lower()
                
                # Standardize units
                unit_mapping = {
                    'bbl': 'bbl',
                    'barrel': 'bbl',
                    'barrels': 'bbl',
                    'bbls': 'bbl',
                    'mcf': 'mcf',
                    'mmcf': 'mmcf',
                    'boe': 'boe',
                }
                
                if unit in unit_mapping:
                    return f"{value} {unit_mapping[unit]}"
                return volume
            except ValueError:
                pass
                
        return volume
    
    def _clean_text(self, text: str) -> str:
        """Clean text content."""
        if not text or not isinstance(text, str):
            return text
            
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Fix common OCR errors in oil & gas terminology
        corrections = {
            "0il": "Oil",
            "0ffshore": "Offshore",
            "C0ntract": "Contract",
            "Petr0leum": "Petroleum",
        }
        
        for error, correction in corrections.items():
            text = text.replace(error, correction)
            
        return text.strip()

=== End of ./core/pipeline/cleaning/deal.py ===


=== File: ./core/pipeline/cleaning/email.py ===
# clm_system/core/pipeline/cleaning/email.py
import re
from html import unescape
from bs4 import BeautifulSoup
from .base import CleanerABC

class EmailCleaner(CleanerABC):
    def clean(self, data: dict) -> dict:
        """
        Clean and normalize email data.
        
        Args:
            data: Raw email data
            
        Returns:
            Cleaned email data
        """
        cleaned = data.copy()
        
        # Clean email addresses
        for field in ["from", "to", "cc", "bcc"]:
            if "metadata" in cleaned and field in cleaned["metadata"]:
                if isinstance(cleaned["metadata"][field], list):
                    cleaned["metadata"][field] = [
                        self._normalize_email(email) for email in cleaned["metadata"][field]
                    ]
                else:
                    cleaned["metadata"][field] = self._normalize_email(cleaned["metadata"][field])
        
        # Clean title/subject
        if "title" in cleaned:
            cleaned["title"] = self._clean_subject(cleaned["title"])
            
        # Clean clauses text (usually email body)
        if "clauses" in cleaned:
            for clause in cleaned["clauses"]:
                if "text" in clause:
                    # Check if this is HTML content
                    if clause.get("metadata", {}).get("format") == "html" or \
                       (clause["text"].startswith("<") and ">" in clause["text"]):
                        clause["text"] = self._clean_html(clause["text"])
                    else:
                        clause["text"] = self._clean_text(clause["text"])
        
        return cleaned
    
    def _normalize_email(self, email_address):
        """Normalize email addresses."""
        if not email_address:
            return email_address
            
        if isinstance(email_address, str):
            # Convert to lowercase
            email_address = email_address.lower()
            
            # Extract actual email from "Name <email>" format
            match = re.search(r'<([^>]+)>', email_address)
            if match:
                email_address = match.group(1)
                
            # Remove whitespace
            email_address = email_address.strip()
            
        return email_address
    
    def _clean_subject(self, subject):
        """Clean email subject/title."""
        if not subject:
            return subject
            
        # Remove common prefixes
        prefixes = ["RE:", "FW:", "FWD:", "Re:", "Fw:", "Fwd:"]
        cleaned_subject = subject
        for prefix in prefixes:
            if cleaned_subject.startswith(prefix):
                cleaned_subject = cleaned_subject[len(prefix):].strip()
                
        # Remove excessive whitespace
        cleaned_subject = re.sub(r'\s+', ' ', cleaned_subject)
        
        return cleaned_subject.strip()
    
    def _clean_html(self, html_content):
        """Convert HTML content to plain text."""
        if not html_content:
            return ""
            
        try:
            # Parse HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove scripts, styles, and hidden divs
            for tag in soup(["script", "style", "meta", "head"]):
                tag.extract()
                
            # Get text content
            text = soup.get_text(separator=' ')
            
            # Remove excessive whitespace
            text = re.sub(r'\s+', ' ', text)
            
            # Replace HTML entities
            text = unescape(text)
            
            # Remove email signature markers and footers
            signature_markers = ["--", "Best regards", "Regards", "Thanks,", "Thank you,", "Sincerely,"]
            for marker in signature_markers:
                if marker in text:
                    # Find position and truncate (keeping some context)
                    pos = text.find(marker)
                    # Keep the marker but limit what comes after
                    signature_limit = min(pos + 100, len(text))
                    text = text[:signature_limit]
                    break
                    
            return text.strip()
        except Exception:
            # If HTML parsing fails, fall back to simple cleanup
            return self._clean_text(html_content)
    
    def _clean_text(self, text):
        """Clean plain text email content."""
        if not text:
            return ""
            
        # Remove excessive line breaks
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # Remove common email quotes (lines starting with >)
        lines = text.split('\n')
        cleaned_lines = [line for line in lines if not line.strip().startswith('>')]
        text = '\n'.join(cleaned_lines)
        
        # Remove common signature separators
        text = re.sub(r'-{2,}|_{2,}|={2,}', '', text)
        
        # Remove common email footers
        footer_patterns = [
            r'Sent from my iPhone',
            r'Sent from my mobile device',
            r'Get Outlook for',
            r'CONFIDENTIALITY NOTICE:',
            r'DISCLAIMER:',
            r'PRIVILEGED AND CONFIDENTIAL',
        ]
        
        for pattern in footer_patterns:
            match = re.search(pattern, text)
            if match:
                text = text[:match.start()]
                
        return text.strip()

=== End of ./core/pipeline/cleaning/email.py ===


=== File: ./core/pipeline/cleaning/recap.py ===
# clm_system/core/pipeline/cleaning/recap.py
import re
from .base import CleanerABC

class RecapCleaner(CleanerABC):
    def clean(self, data: dict) -> dict:
        """
        Clean and normalize recap/summary data.
        
        Args:
            data: Raw recap data
            
        Returns:
            Cleaned recap data
        """
        cleaned = data.copy()
        
        # Normalize recap type
        if "metadata" in cleaned and "recap_type" in cleaned["metadata"]:
            cleaned["metadata"]["recap_type"] = cleaned["metadata"]["recap_type"].lower().replace(" ", "_")
        
        # Clean title
        if "title" in cleaned:
            cleaned["title"] = self._clean_title(cleaned["title"])
        
        # Clean participant names if present
        if "metadata" in cleaned and "participants" in cleaned["metadata"]:
            if isinstance(cleaned["metadata"]["participants"], list):
                cleaned["metadata"]["participants"] = [
                    self._clean_name(participant) for participant in cleaned["metadata"]["participants"]
                ]
        
        # Clean author name
        if "metadata" in cleaned and "author" in cleaned["metadata"]:
            cleaned["metadata"]["author"] = self._clean_name(cleaned["metadata"]["author"])
                
        # Clean clauses text
        if "clauses" in cleaned:
            for clause in cleaned["clauses"]:
                if "text" in clause:
                    clause["text"] = self._clean_text(clause["text"])
                if "title" in clause:
                    clause["title"] = self._clean_title(clause["title"])
        
        return cleaned
    
    def _clean_title(self, title):
        """Clean recap title."""
        if not title:
            return title
            
        # Remove unnecessary prefixes
        prefixes = ["RECAP:", "SUMMARY:", "MINUTES:", "Meeting:"]
        for prefix in prefixes:
            if title.startswith(prefix):
                title = title[len(prefix):].strip()
        
        # Remove excessive whitespace
        title = re.sub(r'\s+', ' ', title)
        
        # Ensure first letter is capitalized
        if title:
            title = title[0].upper() + title[1:]
            
        return title.strip()
    
    def _clean_name(self, name):
        """Clean person names."""
        if not name or not isinstance(name, str):
            return name
            
        # Remove titles
        titles = ["Mr.", "Mrs.", "Ms.", "Dr.", "Prof."]
        for title in titles:
            if name.startswith(title):
                name = name[len(title):].strip()
        
        # Remove parenthetical information (like departments)
        name = re.sub(r'\([^)]*\)', '', name)
        
        # Normalize capitalization
        name = name.title()
        
        return name.strip()
    
    def _clean_text(self, text):
        """Clean recap text content."""
        if not text or not isinstance(text, str):
            return text
            
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove page numbers and headers/footers
        text = re.sub(r'Page \d+ of \d+', '', text)
        text = re.sub(r'CONFIDENTIAL', '', text, flags=re.IGNORECASE)
        
        # Remove bullet point decorations but keep content
        text = re.sub(r'•\s*', '- ', text)
        text = re.sub(r'★\s*', '- ', text)
        text = re.sub(r'✓\s*', '- ', text)
        
        # Fix common OCR errors
        corrections = {
            "0ption": "Option",
            "decisíon": "decision",
            "actíon": "action",
            "revíew": "review",
        }
        
        for error, correction in corrections.items():
            text = text.replace(error, correction)
            
        return text.strip()

=== End of ./core/pipeline/cleaning/recap.py ===


=== File: ./core/pipeline/ingestion/base.py ===
# file: core/pipeline/ingestion/base.py 
from abc import ABC, abstractmethod

class IngestorABC(ABC):
    @abstractmethod
    def ingest(self, raw: dict) -> dict:
        pass


=== End of ./core/pipeline/ingestion/base.py ===


=== File: ./core/pipeline/ingestion/contract.py ===
# clm_system/core/pipeline/ingestion/contract.py
import uuid
from datetime import datetime
from .base import IngestorABC

class ContractIngestor(IngestorABC):
    def ingest(self, raw: dict) -> dict:
        
        data = raw.copy()
        data.setdefault("id", str(uuid.uuid4()))
        now = datetime.utcnow()
        data["created_at"] = now
        data["updated_at"] = now
        return data


=== End of ./core/pipeline/ingestion/contract.py ===


=== File: ./core/pipeline/ingestion/deal.py ===
# clm_system/core/pipeline/ingestion/deal.py
import uuid
from datetime import datetime
from .base import IngestorABC

class DealIngestor(IngestorABC):
    def ingest(self, raw: dict) -> dict:
        """
        Process raw oil industry deal data into a standardized format.
        
        Args:
            raw: Raw deal data from input source
            
        Returns:
            Standardized deal document with metadata
        """
        data = raw.copy()
        
        # Ensure required fields
        data.setdefault("id", str(uuid.uuid4()))
        now = datetime.utcnow()
        data.setdefault("created_at", now)
        data.setdefault("updated_at", now)
        
        # Ensure deal-specific metadata
        if "metadata" not in data:
            data["metadata"] = {}
            
        # Set document type if not present
        data["metadata"].setdefault("document_type", "deal")
        
        # Ensure deal-specific fields
        data["metadata"].setdefault("deal_type", data.get("deal_type", "unspecified"))
        data["metadata"].setdefault("volume", data.get("volume"))
        data["metadata"].setdefault("price_per_unit", data.get("price_per_unit"))
        data["metadata"].setdefault("total_value", data.get("total_value"))
        data["metadata"].setdefault("location", data.get("location"))
        data["metadata"].setdefault("counterparties", data.get("counterparties", []))
        
        return data

=== End of ./core/pipeline/ingestion/deal.py ===


=== File: ./core/pipeline/ingestion/email.py ===
# clm_system/core/pipeline/ingestion/email.py
import uuid
from datetime import datetime
from email.utils import parsedate_to_datetime
from .base import IngestorABC

class EmailIngestor(IngestorABC):
    def ingest(self, raw: dict) -> dict:
        """
        Process raw email data into a standardized format.
        
        Args:
            raw: Raw email data from input source
            
        Returns:
            Standardized email document with metadata
        """
        data = raw.copy()
        
        # Ensure required fields
        data.setdefault("id", str(uuid.uuid4()))
        now = datetime.utcnow()
        data.setdefault("created_at", now)
        data.setdefault("updated_at", now)
        
        # Set title to email subject if not specified
        if "subject" in data and "title" not in data:
            data["title"] = data["subject"]
        
        # Ensure metadata exists
        if "metadata" not in data:
            data["metadata"] = {}
        
        # Set document type
        data["metadata"]["document_type"] = "email"
        
        # Extract and normalize email metadata
        data["metadata"].setdefault("from", data.get("from"))
        data["metadata"].setdefault("to", data.get("to", []))
        data["metadata"].setdefault("cc", data.get("cc", []))
        data["metadata"].setdefault("bcc", data.get("bcc", []))
        
        # Parse email date if present
        if "date" in data and isinstance(data["date"], str):
            try:
                parsed_date = parsedate_to_datetime(data["date"])
                data["metadata"]["email_date"] = parsed_date
            except Exception:
                # If parsing fails, use ingestion date
                data["metadata"]["email_date"] = now
        
        # Handle email body
        if "body" in data:
            # Store original body in metadata
            data["metadata"]["original_format"] = "text" if isinstance(data["body"], str) else "html"
            
            # Convert body to clauses
            if "clauses" not in data:
                data["clauses"] = [{
                    "id": f"{data['id']}_body",
                    "type": "email_body",
                    "text": data["body"],
                    "position": 0
                }]
            
            # Remove body from top level after storing in clauses
            data.pop("body", None)
            
        # Handle attachments metadata
        if "attachments" in data:
            data["metadata"]["has_attachments"] = True
            data["metadata"]["attachment_count"] = len(data["attachments"])
            data["metadata"]["attachment_names"] = [
                att.get("filename", f"attachment_{i}") 
                for i, att in enumerate(data["attachments"])
            ]
            # Remove attachments from top level
            data.pop("attachments", None)
        else:
            data["metadata"]["has_attachments"] = False
            
        return data

=== End of ./core/pipeline/ingestion/email.py ===


=== File: ./core/pipeline/ingestion/recap.py ===
# clm_system/core/pipeline/ingestion/recap.py
import uuid
from datetime import datetime
from .base import IngestorABC

class RecapIngestor(IngestorABC):
    def ingest(self, raw: dict) -> dict:
        """
        Process raw recap/summary data into a standardized format.
        
        Args:
            raw: Raw recap data from input source
            
        Returns:
            Standardized recap document with metadata
        """
        data = raw.copy()
        
        # Ensure required fields
        data.setdefault("id", str(uuid.uuid4()))
        now = datetime.utcnow()
        data.setdefault("created_at", now)
        data.setdefault("updated_at", now)
        
        # Ensure metadata exists
        if "metadata" not in data:
            data["metadata"] = {}
            
        # Set document type
        data["metadata"]["document_type"] = "recap"
        
        # Extract recap-specific metadata
        data["metadata"].setdefault("recap_type", data.get("recap_type", "general"))
        data["metadata"].setdefault("meeting_date", data.get("meeting_date"))
        data["metadata"].setdefault("author", data.get("author"))
        data["metadata"].setdefault("participants", data.get("participants", []))
        data["metadata"].setdefault("related_documents", data.get("related_documents", []))
        
        # Handle content sections
        if "sections" in data and isinstance(data["sections"], list):
            # Convert sections to clauses if not already present
            if "clauses" not in data:
                data["clauses"] = []
                for i, section in enumerate(data["sections"]):
                    if isinstance(section, dict):
                        data["clauses"].append({
                            "id": f"{data['id']}_section_{i}",
                            "title": section.get("heading", f"Section {i+1}"),
                            "type": section.get("type", "recap_section"),
                            "text": section.get("content", ""),
                            "position": i,
                            "metadata": {
                                "section_type": section.get("type", "general")
                            }
                        })
            # Remove sections from top level after storing in clauses
            data.pop("sections", None)
            
        return data

=== End of ./core/pipeline/ingestion/recap.py ===


=== File: ./core/pipeline/orchestrator.py ===
# clm_system/core/pipeline/orchestrator.py
import asyncio
import logging
from typing import Dict, Any, List, Tuple, Optional

from clm_system.core.pipeline.ingestion.contract import ContractIngestor
from clm_system.core.pipeline.ingestion.deal import DealIngestor
from clm_system.core.pipeline.ingestion.email import EmailIngestor
from clm_system.core.pipeline.ingestion.recap import RecapIngestor

from clm_system.core.pipeline.cleaning.contract import ContractCleaner
from clm_system.core.pipeline.cleaning.deal import DealCleaner
from clm_system.core.pipeline.cleaning.email import EmailCleaner
from clm_system.core.pipeline.cleaning.recap import RecapCleaner

from clm_system.core.pipeline.chunking.contract import ContractChunker
from clm_system.core.pipeline.chunking.deal import DealChunker
from clm_system.core.pipeline.chunking.email import EmailChunker

from clm_system.core.database.mongodb_client import MongoDBClient
from clm_system.core.database.elasticsearch_client import ElasticsearchClient
from clm_system.core.database.qdrant_client import QdrantClient
from clm_system.core.utils.embeddings import get_embedding_model, compute_embedding

logger = logging.getLogger(__name__)

class PipelineService:
    def __init__(self):
        # Document type registry with processing components
        self.ingestors = {
            "contract": ContractIngestor(),
            "deal": DealIngestor(),
            "email": EmailIngestor(),
            "recap": RecapIngestor(),
        }
        
        self.cleaners = {
            "contract": ContractCleaner(),
            "deal": DealCleaner(),
            "email": EmailCleaner(),
            "recap": RecapCleaner(),
        }
        
        # Note: No RecapChunker since recaps are a component of deals
        self.chunkers = {
            "contract": ContractChunker(),
            "deal": DealChunker(),
            "email": EmailChunker(),
        }
        
        # Initialize database connections
        self.mongo = MongoDBClient()
        self.es = ElasticsearchClient()
        self.qdrant = QdrantClient()
        
        # Load embedding model
        self.embedding_model = get_embedding_model()

    async def process_document(self, raw: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main pipeline execution flow:
        1. Determine document type from metadata
        2. Run through type-specific processing components
        3. Store in databases and vector store
        4. Return processing results
        """
        try:
            # Determine document type (default to contract if missing)
            doc_type = raw.get("metadata", {}).get("document_type", "contract")
            
            # Validate document type
            if doc_type not in self.ingestors:
                raise ValueError(f"Unsupported document type: {doc_type}")
            
            # Get processing components
            ingestor = self.ingestors[doc_type]
            cleaner = self.cleaners[doc_type]
            
            # Pipeline execution - steps 1 & 2
            normalized = ingestor.ingest(raw)      # Structure raw data
            cleaned = cleaner.clean(normalized)    # Sanitize and normalize
            
            # Special handling for recaps - don't chunk them
            if doc_type == "recap":
                # Store recaps without chunking
                await self._persist_document(cleaned)
                
                logger.info(f"Pipeline complete for recap {cleaned['id']}")
                return {
                    "id": cleaned["id"],
                    "title": cleaned.get("title", "Untitled Recap"),
                    "status": "indexed",
                    "document_type": doc_type
                }
            else:
                # For other document types, proceed with chunking
                chunker = self.chunkers[doc_type]
                
                # Step 3: Chunk & embed based on document type structure
                chunks = self._generate_chunks_for_type(cleaned, chunker, doc_type)
                
                # Step 4: Persist in MongoDB and Elasticsearch
                await self._persist_document(cleaned)
                
                # Step 5: Store embeddings in Qdrant if there are chunks
                if chunks:
                    await self._store_embeddings(cleaned, chunks, doc_type)
                
                logger.info(f"Pipeline complete for {doc_type} {cleaned['id']}")
                
                # Return appropriate result based on document type
                result = {
                    "id": cleaned["id"],
                    "title": cleaned.get("title", f"Untitled {doc_type.title()}"),
                    "status": "indexed",
                    "document_type": doc_type
                }
                
                # Add type-specific fields
                if doc_type == "contract" and "clauses" in cleaned:
                    result["clauses_count"] = len(cleaned["clauses"])
                elif doc_type == "email":
                    if "metadata" in cleaned and "has_attachments" in cleaned["metadata"]:
                        result["has_attachments"] = cleaned["metadata"]["has_attachments"]
                elif doc_type == "deal":
                    if "metadata" in cleaned and "deal_type" in cleaned["metadata"]:
                        result["deal_type"] = cleaned["metadata"]["deal_type"]
                
                return result
            
        except Exception as e:
            logger.error(f"Document processing failed: {str(e)}")
            raise
        finally:
            await self._cleanup_connections()

    async def _cleanup_connections(self):
        """Clean up all database connections"""
        if hasattr(self.mongo, 'client'):
            self.mongo.client.close()
        if hasattr(self.es, 'client'):
            await self.es.client.close()
        if hasattr(self.qdrant, 'client'):
            await self.qdrant.client.close()
    def _generate_chunks_for_type(self, 
                                  document: Dict[str, Any], 
                                  chunker: Any, 
                                  doc_type: str) -> List[Tuple[Dict[str, Any], str, List[float]]]:
        """Generate chunks based on document type structure"""
        chunks = []
        
        if doc_type == "contract":
            # Contracts have clauses
            for clause in document.get("clauses", []):
                for text in chunker.chunk(clause["text"]):
                    emb = compute_embedding(text, self.embedding_model)
                    chunks.append((clause, text, emb))
                    
        elif doc_type == "email":
            # Emails may have a body or content field instead of clauses
            if "body" in document:
                # Email with direct body field
                for text in chunker.chunk(document["body"]):
                    emb = compute_embedding(text, self.embedding_model)
                    # Create a pseudo-clause for consistent handling
                    pseudo_clause = {
                        "id": f"{document['id']}_body",
                        "type": "email_body",
                        "metadata": {}
                    }
                    chunks.append((pseudo_clause, text, emb))
            elif "content" in document:
                # Email with content field
                for text in chunker.chunk(document["content"]):
                    emb = compute_embedding(text, self.embedding_model)
                    pseudo_clause = {
                        "id": f"{document['id']}_content",
                        "type": "email_content",
                        "metadata": {}
                    }
                    chunks.append((pseudo_clause, text, emb))
            elif "clauses" in document:
                # Email already processed into clauses
                for clause in document["clauses"]:
                    for text in chunker.chunk(clause["text"]):
                        emb = compute_embedding(text, self.embedding_model)
                        chunks.append((clause, text, emb))
                        
        elif doc_type == "deal":
            # Deals may have sections or clauses
            if "sections" in document:
                for i, section in enumerate(document["sections"]):
                    if isinstance(section, dict) and "content" in section:
                        for text in chunker.chunk(section["content"]):
                            emb = compute_embedding(text, self.embedding_model)
                            pseudo_clause = {
                                "id": f"{document['id']}_section_{i}",
                                "type": section.get("type", "deal_section"),
                                "metadata": {
                                    "section_title": section.get("title", f"Section {i+1}")
                                }
                            }
                            chunks.append((pseudo_clause, text, emb))
            elif "clauses" in document:
                # Deal already processed into clauses
                for clause in document["clauses"]:
                    for text in chunker.chunk(clause["text"]):
                        emb = compute_embedding(text, self.embedding_model)
                        chunks.append((clause, text, emb))
            elif "terms" in document and isinstance(document["terms"], list):
                # Deal with terms structure
                for i, term in enumerate(document["terms"]):
                    if isinstance(term, dict) and "description" in term:
                        for text in chunker.chunk(term["description"]):
                            emb = compute_embedding(text, self.embedding_model)
                            pseudo_clause = {
                                "id": f"{document['id']}_term_{i}",
                                "type": "deal_term",
                                "metadata": {
                                    "term_name": term.get("name", f"Term {i+1}")
                                }
                            }
                            chunks.append((pseudo_clause, text, emb))
                            
        return chunks
    
    async def _persist_document(self, document: Dict[str, Any]):
        """Store document in MongoDB and Elasticsearch"""
        # MongoDB - main document storage
        # Ensure async connections
        await self.mongo.client.admin.command('ping')
        await self.es.client.info()
        
        # MongoDB - main document storage
        inserted_id = await self.mongo.insert_contract(document)
        logger.debug("Inserted MongoDB ID: %s", inserted_id)
        
        # Elasticsearch - searchable content
        es_doc = document.copy()
        es_doc.pop("_id", None)
        es_response = await self.es.index_contract(es_doc)
        logger.debug("ES response: %s", es_response)
        
    async def _store_embeddings(self, 
                               document: Dict[str, Any], 
                               chunks: List[Tuple[Dict[str, Any], str, List[float]]], 
                               doc_type: str):
        """Store embeddings in vector database"""
        for clause, text, emb in chunks:
            # Get metadata from the document and clause
            doc_metadata = document.get("metadata", {})
            clause_metadata = clause.get("metadata", {})
            
            # Combine metadata, with clause metadata taking precedence
            combined_metadata = {**doc_metadata, **clause_metadata}
            
            # Add document type to metadata
            combined_metadata["document_type"] = doc_type
            
            await self.qdrant.store_embedding(
                contract_id=document["id"],
                contract_title=document.get("title", f"Untitled {doc_type.title()}"),
                clause_id=clause["id"],
                clause_type=clause.get("type", "generic"),
                content=text,
                metadata=combined_metadata,
                embedding=emb
            )

=== End of ./core/pipeline/orchestrator.py ===


=== File: ./core/pipeline/preprocessing/pdf_processor.py ===
# clm_system/core/preprocessing/pdf_processor.py
import fitz  # PyMuPDF
import re
import logging
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

class PDFProcessor:
    """Processes PDF files into contract JSON structure."""
    
    def __init__(self):
        # Initialize any needed resources
        pass
        
    async def process_pdf(self, file_path: str) -> Dict[str, Any]:
        """
        Process a PDF file into a contract structure.
        
        Args:
            file_path: Path to the PDF file
            
        Returns:
            Contract object in dictionary form
        """
        try:
            # Extract text from PDF
            text = await self._extract_text(file_path)
            logger.debug(f"Extracted raw text:\n{text[:1000]}...")  # First 1000 chars
            # Process the text into contract structure
            contract = await self._structure_contract(text, file_path)
            logger.debug("Structured contract:", contract)
            
            return contract
        except Exception as e:
            logger.error(f"Error processing PDF {file_path}: {e}")
            raise
    
    async def _extract_text(self, file_path: str) -> str:
        """Extract and normalize text from PDF."""
        text = ""
        try:
            doc = fitz.open(file_path)
            for page in doc:
                text += page.get_text("text", flags=fitz.TEXT_PRESERVE_LIGATURES)
            
            # Normalize text
            text = re.sub(r'\s+', ' ', text)  # Replace multiple whitespace
            text = re.sub(r'(\w)-\n(\w)', r'\1\2', text)  # Fix hyphenated words
            return text
        except Exception as e:
            logger.error(f"Error extracting text from PDF: {e}")
            raise
    
    async def _structure_contract(self, text: str, file_name: str) -> Dict[str, Any]:
        """
        Convert raw text into contract structure.
        Uses heuristics and patterns to identify contract components.
        """
        # Basic structure
        contract = {
            "title": self._extract_title(text, file_name),
            "metadata": self._extract_metadata(text),
            "clauses": self._extract_clauses(text)
        }
        logger.debug(f"Structured contract: {contract}")
        return contract
    
    def _extract_title(self, text: str, file_name: str) -> str:
        """Extract contract title from text or use filename."""
        # Look for title patterns (often in first few lines)
        first_lines = text.strip().split('\n')[:5]
        for line in first_lines:
            if re.search(r'agreement|contract|terms', line.lower()):
                return line.strip()
        
        # Fallback to filename
        base_name = os.path.basename(file_name)
        name_without_ext = os.path.splitext(base_name)[0]
        return name_without_ext.replace('_', ' ').title()
    
    def _extract_metadata(self, text: str) -> Dict[str, Any]:
        """Extract contract metadata."""
        metadata = {
            "contract_type": self._identify_contract_type(text),
            "parties": self._identify_parties(text),
            "status": "draft",  # Default status
            "tags": []  # Can be filled based on content analysis
        }
        
        # Extract dates
        dates = self._extract_dates(text)
        if dates.get("effective_date"):
            metadata["effective_date"] = dates["effective_date"]
        if dates.get("expiration_date"):
            metadata["expiration_date"] = dates["expiration_date"]
            
        return metadata
    
    def _identify_contract_type(self, text: str) -> str:
        """Identify contract type from text."""
        type_patterns = {
            "license": r'license|licensing|licensor|licensee',
            "service": r'service|services|provider|customer',
            "nda": r'confidential|non-disclosure|nda',
            "employment": r'employment|employer|employee|hire',
            "purchase": r'purchase|procurement|buyer|seller'
        }
        
        for contract_type, pattern in type_patterns.items():
            if re.search(pattern, text.lower()):
                return contract_type
                
        return "other"
    
    def _identify_parties(self, text: str) -> List[Dict[str, str]]:
        """Extract party information."""
        parties = []
        
        # Look for common party patterns
        party_patterns = [
            r'between\s+([^,]+),?\s+(?:a|an)\s+([^,]+)',
            r'(?:party|client|customer|vendor|supplier):\s*([^\n]+)',
            r'(?:hereinafter\s+referred\s+to\s+as\s+["\'])([^"\']+)'
        ]
        
        for pattern in party_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for i, match in enumerate(matches):
                party_name = match.group(1).strip()
                if party_name and len(party_name) < 100:  # Sanity check
                    parties.append({
                        "name": party_name,
                        "id": f"party-{len(parties)+1:03d}"
                    })
        
        return parties
    
    from dateutil import parser  # Add this import

    def _extract_dates(self, text: str) -> Dict[str, str]:
        dates = {}
        
        # Existing regex patterns
        effective_match = re.search(r'effective\s+date.*?(\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{2,4})', text, re.IGNORECASE)
        expiration_match = re.search(r'(?:expiration|termination|end)\s+date.*?(\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{2,4})', text, re.IGNORECASE)

        # Parse dates with flexible format handling
        try:
            if effective_match:
                dates["effective_date"] = parser.parse(effective_match.group(1)).isoformat()
        except:
            logger.warning(f"Could not parse effective date: {effective_match.group(1)}")

        try:
            if expiration_match:
                dates["expiration_date"] = parser.parse(expiration_match.group(1)).isoformat()
        except:
            logger.warning(f"Could not parse expiration date: {expiration_match.group(1)}")

        return dates
    
    def _extract_clauses(self, text: str) -> List[Dict[str, Any]]:
        """Extract clauses from contract text."""
        clauses = []
        logger.debug(f"Raw text for clause extraction:\n{text[:2000]}...")  # First 2000 chars
        
        # Enhanced section pattern with multiple variations
        section_pattern = r'''
            (?:\n|\r\n)  # Section starts with newline
            (?:          # Section number formats:
            (?:\d+[\.\)]?|          # 1, 2., 3)
            [A-Z][\.\)]|           # A., B)
            ARTICLE\s+[IVXLCDM]+|  # ARTICLE I
            SECTION\s+[\dA-Z]+|    # SECTION 1, SECTION A
            Clause\s+\d+|          # Clause 1
            §\s?[\dA-Z]+|          # §1, §A
            [IVXLCDM]+[\.\)]       # I., II)
            )
            )
            \s+          # Whitespace after number
            ([A-Z][^\.:\n]{5,})  # Title (capitalized, min 5 chars)
            [\.:]?       # Optional ending punctuation
            (?=\n|\r\n|$)  # Lookahead for newline or end
        '''
        
        sections = re.split(section_pattern, text, flags=re.IGNORECASE|re.VERBOSE)
        
        # Debug found sections
        logger.debug(f"Split sections: {sections[:10]}")  # First 10 sections
        
        position = 1
        for i in range(1, len(sections)-1, 2):
            title = sections[i].strip()
            content = sections[i+1].strip()
            
            if len(content) > 50:  # Increased minimum content length
                clause_type = self._identify_clause_type(title, content)
                clauses.append({
                    "id": f"clause-{position:03d}",
                    "title": title,
                    "type": clause_type,
                    "text": content,
                    "position": position
                })
                position += 1
                
        # Fallback: Split by paragraph if no sections found
        if not clauses:
            logger.warning("No sections found, falling back to paragraph split")
            paragraphs = [p.strip() for p in re.split(r'\n{2,}', text) if len(p.strip()) > 100]
            for i, para in enumerate(paragraphs):
                clauses.append({
                    "id": f"para-{i+1:03d}",
                    "title": f"Paragraph {i+1}",
                    "type": "uncategorized",
                    "text": para,
                    "position": i+1
                })
        
        logger.debug(f"Found {len(clauses)} clauses: {clauses}")
        logger.debug(f"Pre-filtered clauses: {len(clauses)} items")
        valid_clauses = [c for c in clauses if c.get("text")]
        logger.debug(f"Post-filtered clauses: {len(valid_clauses)} items")
                
        return [
        clause for clause in clauses
        if clause.get("text") and len(clause["text"]) > 50
    ]

=== End of ./core/pipeline/preprocessing/pdf_processor.py ===


=== File: ./core/query_engine/helpers.py ===
from typing import List, Dict 

def reciprocal_rank_fusion(
    results_a: List[Dict],
    results_b: List[Dict],
    k: int = 60,
    weight_a: float = 1.0,
    weight_b: float = 1.0
) -> List[Dict]:
    """
    Combines search results using Reciprocal Rank Fusion algorithm.
    """
    fused_results = {}
    
    # Process first result list
    for idx, item in enumerate(results_a):
        doc_id = f"{item['contract_id']}_{item['clause_id']}"
        rank = idx + 1
        score = weight_a * (1 / (k + rank))
        fused_results[doc_id] = {
            **item,
            "rrf_score": score,
            "origin": "elastic"
        }
    
    # Process second result list and update scores
    for idx, item in enumerate(results_b):
        doc_id = f"{item['contract_id']}_{item['clause_id']}"
        rank = idx + 1
        score = weight_b * (1 / (k + rank))
        
        if doc_id in fused_results:
            # Document exists in both lists, merge and update score
            fused_results[doc_id]["rrf_score"] += score
            fused_results[doc_id]["origin"] = "both"
            # Keep metadata from both if they differ
            if item.get("metadata") != fused_results[doc_id].get("metadata"):
                fused_results[doc_id]["metadata"] = {
                    **fused_results[doc_id].get("metadata", {}),
                    **item.get("metadata", {})
                }
        else:
            # New document, add to results
            fused_results[doc_id] = {
                **item,
                "rrf_score": score,
                "origin": "vector"
            }
    
    # Convert back to list and sort by RRF score
    combined = list(fused_results.values())
    sorted_results = sorted(combined, key=lambda x: -x["rrf_score"])
    
    # Normalize scores to 0-1 range
    if sorted_results:
        max_score = max(r["rrf_score"] for r in sorted_results)
        if max_score > 0:
            for r in sorted_results:
                r["relevance_score"] = r["rrf_score"] / max_score
                del r["rrf_score"]
                del r["origin"]  # Clean up temp field
    
    return sorted_results

=== End of ./core/query_engine/helpers.py ===


=== File: ./core/query_engine/query_classifier.py ===
import logging
import time
from typing import Optional
import asyncio
from openai import AsyncOpenAI, APIError, RateLimitError
from clm_system.config import settings

logger = logging.getLogger(__name__)

class QueryClassifier:
    def __init__(self):
        #self.client = AsyncOpenAI(api_key=settings.openai_api_key)
        self.client = AsyncOpenAI(
            base_url="http://localhost:1234/v1",  # Default LM Studio port
            api_key="deepseek-r1-distill-qwen-7b"  # Dummy key required by client
        )
        self.cache = {}  # Simple cache for demo purposes
        self.cache_ttl = 3600  # 1 hour TTL
        self.cache_timestamps = {}

    async def classify(self, query: str) -> str:
        # Check cache and TTL
        if query in self.cache:
            timestamp = self.cache_timestamps.get(query, 0)
            if time.time() - timestamp < self.cache_ttl:
                return self.cache[query]
        
        # Fallback classification in case API fails
        fallback = self._heuristic_classify(query)
        
        max_retries = 3
        retry_delay = 1.0
        
        for attempt in range(max_retries):
            try:
                response = await self.client.chat.completions.create(
                    #model="gpt-3.5-turbo",
                    model="local-model",
                    messages=[{
                        "role": "system",
                        "content": """Classify legal contract search queries. Respond with ONE word:
                        - 'structured' for exact filters/terms (e.g., "contracts with effective date after 2023")
                        - 'semantic' for conceptual/meaning-based (e.g., "liability clauses protecting against data breaches")
                        - 'hybrid' for mixed queries (e.g., "confidentiality provisions in NDAs signed after January")"""
                    }, {
                        "role": "user",
                        "content": query
                    }],
                    temperature=0.1,
                    max_tokens=10
                )

                classification = response.choices[0].message.content.lower().strip()
                valid = {"structured", "semantic", "hybrid"}
                result = classification if classification in valid else fallback
                
                # Update cache
                self.cache[query] = result
                self.cache_timestamps[query] = time.time()
                return result
                
            except RateLimitError:
                logger.warning(f"Rate limit exceeded on attempt {attempt+1}, retrying in {retry_delay}s")
                await asyncio.sleep(retry_delay)
                retry_delay *= 2  # Exponential backoff
                
            except APIError as e:
                logger.error(f"OpenAI API error: {str(e)}")
                return fallback
                
            except Exception as e:
                logger.error(f"Classification failed: {str(e)}")
                return fallback
        
        # If all retries failed
        return fallback
    
    def _heuristic_classify(self, query: str) -> str:
        """Fallback classification using heuristics."""
        structured_keywords = [
            "date:", "type:", "status:", "party:", "before:", "after:",
            "contract type", "effective date", "expiration date", "status is"
        ]
        
        has_structured = any(keyword in query.lower() for keyword in structured_keywords)
        
        if len(query.split()) <= 3 and not has_structured:
            return "semantic"
        
        if len(query.split()) > 3 and has_structured:
            return "hybrid"
        
        if has_structured:
            return "structured"
        
        return "semantic"

=== End of ./core/query_engine/query_classifier.py ===


=== File: ./core/query_engine/search.py ===
# File: clm_system/core/queryEngine/search.py
import asyncio
import logging
import time
from typing import Dict, List, Optional, Any, Union

from clm_system.config import settings
from clm_system.core.database.elasticsearch_client import ElasticsearchClient
from clm_system.core.database.qdrant_client import QdrantClient
from clm_system.core.utils.embeddings import get_embedding_model, compute_embedding
from .query_classifier import QueryClassifier
from .helpers import reciprocal_rank_fusion  # Add this import

logger = logging.getLogger(__name__)

class QueryRouter:
    """
    Routes queries to either structured search (Elasticsearch) or 
    semantic search (Qdrant) based on query analysis.
    """
    
    def __init__(self):
        self.es_client = ElasticsearchClient()
        self.qdrant_client = QdrantClient()
        self.embedding_model = get_embedding_model()
        self.top_k = settings.default_top_k
        self.classifier = QueryClassifier()
    
    async def route_query(
        self, query: str, filters: Optional[Dict[str, Any]] = None, top_k: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Analyzes the query and routes it to the appropriate search engine.
        
        Args:
            query: User's search query
            filters: Optional metadata filters
            top_k: Number of results to return
            
        Returns:
            Dict containing search results and metadata
        """
        start_time = time.time()
        
        if top_k is None:
            top_k = self.top_k
        
        # Determine query type using classifier
        query_type = await self.classifier.classify(query)
        logger.info(f"Query classified as {query_type}: {query}")
        
        results = []
        
        if query_type == "structured":
            # Structured search using Elasticsearch
            results = await self.es_client.search(query, filters, top_k)
        elif query_type == "semantic":
            # Semantic search using Qdrant
            query_embedding = compute_embedding(query, self.embedding_model)
            results = await self.qdrant_client.search(query_embedding, filters, top_k)
        else:  # hybrid
            # Compute embedding here before passing to search
            query_embedding = compute_embedding(query, self.embedding_model)
            
            # Run searches in parallel
            es_results, qdrant_results = await asyncio.gather(
                self.es_client.search(query, filters, top_k * 2),
                self.qdrant_client.search(query_embedding, filters, top_k * 2)
            )
            
            # Combine results using RRF
            results = reciprocal_rank_fusion(
                es_results,
                qdrant_results,
                k=60,
                weight_a=0.4,  # Elasticsearch weight
                weight_b=0.6   # Vector search weight
            )[:top_k]
        
        execution_time = (time.time() - start_time) * 1000  # Convert to milliseconds
        
        return {
            "query": query,
            "total_results": len(results),
            "results": results,
            "metadata": {
                "query_type": query_type,
                "filters_applied": filters is not None
            },
            "execution_time_ms": execution_time
        }
    
    # This method is no longer needed since we're using the QueryClassifier
    # It can be removed or kept as a fallback
    def _heuristic_classify(self, query: str) -> str:
        """
        Classifies a query as structured, semantic, or hybrid using heuristics.
        Used as a fallback when classifier is unavailable.
        
        Args:
            query: The user's search query
            
        Returns:
            String indicating query type: "structured", "semantic", or "hybrid"
        """
        structured_keywords = [
            "date:", "type:", "status:", "party:", "before:", "after:",
            "contract type", "effective date", "expiration date", "status is"
        ]
        
        has_structured = any(keyword in query.lower() for keyword in structured_keywords)
        
        if len(query.split()) <= 3 and not has_structured:
            return "semantic"
        
        if len(query.split()) > 3 and has_structured:
            return "hybrid"
        
        if has_structured:
            return "structured"
        
        return "semantic"

=== End of ./core/query_engine/search.py ===


=== File: ./core/utils/embeddings.py ===


# File: clm_system/core/utils/embeddings.py
import logging
from typing import List, Optional

import torch
from sentence_transformers import SentenceTransformer

from clm_system.config import settings

logger = logging.getLogger(__name__)

# Global cache for embedding model
_embedding_model = None

def get_embedding_model() -> SentenceTransformer:
    """
    Gets or initializes the embedding model.
    
    Returns:
        SentenceTransformer model instance
    """
    global _embedding_model
    
    if _embedding_model is None:
        logger.info(f"Loading embedding model: {settings.embedding_model}")
        try:
            _embedding_model = SentenceTransformer(settings.embedding_model)
            logger.info(f"Embedding model loaded successfully")
        except Exception as e:
            logger.error(f"Error loading embedding model: {str(e)}")
            raise
    
    return _embedding_model

def compute_embedding(text: str, model: Optional[SentenceTransformer] = None) -> List[float]:
    """
    Computes embedding for a given text.
    
    Args:
        text: Input text to embed
        model: Optional pre-loaded model (if not provided, will get from cache)
        
    Returns:
        List of floats representing the text embedding
    """
    if model is None:
        model = get_embedding_model()
    
    try:
        # Compute embedding
        embedding = model.encode(text)
        
        # Convert to list if it's a tensor or numpy array
        if isinstance(embedding, torch.Tensor):
            embedding = embedding.tolist()
        elif hasattr(embedding, "tolist"):
            embedding = embedding.tolist()
        
        return embedding
    except Exception as e:
        logger.error(f"Error computing embedding: {str(e)}")
        raise

=== End of ./core/utils/embeddings.py ===


=== File: ./core/utils/__init__.py ===


=== End of ./core/utils/__init__.py ===


=== File: ./core/__init__.py ===


=== End of ./core/__init__.py ===


=== File: ./main.py ===
# File: clm_system/main.py
import logging
import os

import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from clm_system.api.routes import router as api_router
from clm_system.config import settings

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=getattr(logging, os.getenv("LOG_LEVEL", "INFO")),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(
    title="CLM Smart Search",
    description="Contract Lifecycle Management with Smart Search capabilities",
    version="0.1.0",
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # For production, specify exact origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include API routes
app.include_router(api_router, prefix="/api")

# Health check endpoint
@app.get("/health")
async def health_check():
    return {"status": "ok"}

if __name__ == "__main__":
    logger.info(f"Starting server on {settings.api_host}:{settings.api_port}")
    uvicorn.run(
        "clm_system.main:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=True,
    )



=== End of ./main.py ===


=== File: ./schemas/schemas.py ===
# File: clm_system/api/schemas.py
from pydantic import BaseModel, Field
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
from pydantic import validator

class ClauseBase(BaseModel):
    """Base schema for contract clauses."""
    id: Optional[str] = None
    title: Optional[str] = None
    type: str
    text: str
    position: Optional[int] = None
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)


class ContractMetadata(BaseModel):
    contract_type: str
    effective_date: Optional[datetime] = None
    expiration_date: Optional[datetime] = None
    parties: List[Dict[str, str]] = Field(default_factory=list)
    status: Optional[str] = None
    tags: List[str] = Field(default_factory=list)

    @validator('effective_date', 'expiration_date', pre=True)
    def parse_dates(cls, value):
        if isinstance(value, str):
            try:
                return parser.parse(value)
            except:
                return None
        return value

class ContractCreate(BaseModel):
    """Schema for creating a new contract."""
    title: str
    metadata: ContractMetadata
    clauses: List[ClauseBase]


class ContractResponse(BaseModel):
    """Response schema for contract operations."""
    id: str
    title: str
    metadata: ContractMetadata
    created_at: datetime
    updated_at: datetime
    clauses_count: int
    status: str = "indexed"


class QueryRequest(BaseModel):
    """Schema for search queries."""
    query: str
    filters: Optional[Dict[str, Any]] = None
    top_k: Optional[int] = 5


class ClauseSearchResult(BaseModel):
    """Result schema for clause search."""
    clause_id: str
    contract_id: str
    contract_title: str
    clause_type: str
    clause_title: Optional[str] = None
    content: str
    relevance_score: float
    metadata: Optional[Dict[str, Any]] = None


class QueryResponse(BaseModel):
    """Response schema for search queries."""
    query: str
    total_results: int
    results: List[ClauseSearchResult]
    metadata: Optional[Dict[str, Any]] = None
    execution_time_ms: float


=== End of ./schemas/schemas.py ===


=== File: ./test.py ===
import pytest
import asyncio
from unittest.mock import MagicMock, patch

from clm_system.core.queryEngine.search import QueryRouter

@pytest.fixture
def sample_contract():
    """Sample contract data for testing."""
    return {
        "id": "test-contract-123",
        "title": "Software License Agreement",
        "metadata": {
            "contract_type": "license",
            "effective_date": "2023-01-01T00:00:00Z",
            "expiration_date": "2024-01-01T00:00:00Z",
            "parties": [
                {"name": "ACME Corp", "id": "party-001"},
                {"name": "Supplier Inc", "id": "party-002"}
            ],
            "status": "active",
            "tags": ["software", "license", "annual"]
        },
        "clauses": [
            {
                "id": "clause-001",
                "title": "License Grant",
                "type": "grant",
                "text": "Licensor hereby grants to Licensee a non-exclusive, non-transferable license to use the Software.",
                "position": 1
            },
            {
                "id": "clause-002",
                "title": "Term and Termination",
                "type": "term",
                "text": "This Agreement shall commence on the Effective Date and continue for a period of one (1) year.",
                "position": 2
            }
        ]
    }

@pytest.mark.asyncio
async def test_query_router_classification():
    """Test query classification logic."""
    router = QueryRouter()
    
    # Test structured query classification
    structured_queries = [
        "contracts with effective date after 2023-01-01",
        "status: active type: license",
        "find contracts with party: ACME Corp",
        "contract type: license"
    ]
    
    for query in structured_queries:
        assert router._classify_query(query) in ["structured", "hybrid"], f"Failed for: {query}"
    
    # Test semantic query classification
    semantic_queries = [
        "what are the license terms",
        "termination conditions",
        "find software agreements"
    ]
    
    for query in semantic_queries:
        assert router._classify_query(query) == "semantic", f"Failed for: {query}"

@pytest.mark.asyncio
async def test_route_query():
    """Test query routing to appropriate search engines."""
    # Mock the Elasticsearch and Qdrant clients
    with patch('clm_system.core.search.ElasticsearchClient') as mock_es, \
         patch('clm_system.core.search.QdrantClient') as mock_qdrant, \
         patch('clm_system.core.search.compute_embedding') as mock_compute_embedding:
        
        # Setup mocks
        mock_es_instance = mock_es.return_value
        mock_es_instance.search.return_value = [{"clause_id": "test1"}]
        
        mock_qdrant_instance = mock_qdrant.return_value
        mock_qdrant_instance.search.return_value = [{"clause_id": "test2"}]
        
        mock_compute_embedding.return_value = [0.1] * 384  # Mock embedding
        
        router = QueryRouter()
        router.es_client = mock_es_instance
        router.qdrant_client = mock_qdrant_instance
        
        # Test structured query routing
        structured_result = await router.route_query("contract type: license", {"status": "active"})
        assert structured_result["query_type"] == "structured"
        assert mock_es_instance.search.called
        
        # Test semantic query routing
        semantic_result = await router.route_query("what are termination conditions")
        assert semantic_result["query_type"] == "semantic"
        assert mock_qdrant_instance.search.called
        
        # Test hybrid query routing
        mock_es_instance.search.reset_mock()
        mock_qdrant_instance.search.reset_mock()
        
        hybrid_result = await router.route_query("find active license contracts with termination clause")
        assert hybrid_result["query_type"] == "hybrid"
        assert mock_es_instance.search.called
        assert mock_qdrant_instance.search.called

=== End of ./test.py ===


=== File: ./__init__.py ===


=== End of ./__init__.py ===


