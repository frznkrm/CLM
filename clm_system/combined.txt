
=== File: ./api/routes.py ===
# clm_system/api/routes.py
import logging
import os
import tempfile
from typing import Dict, Any, List, Optional

from fastapi import APIRouter, File, UploadFile, Form, HTTPException, Query
from fastapi.responses import JSONResponse
from pydantic import ValidationError

from clm_system.core.pipeline.orchestrator import PipelineService
from clm_system.core.queryEngine.search import QueryRouter
from clm_system.core.preprocessing.pdf_processor import PDFProcessor
from clm_system.schemas.schemas import ContractCreate

router = APIRouter()
logger = logging.getLogger(__name__)

# Initialize services
pipeline = PipelineService()
query_router = QueryRouter()
pdf_processor = PDFProcessor()

@router.post("/contracts/ingest", response_model=Dict[str, Any])
async def ingest_contract(contract: ContractCreate):
    """
    Ingest a contract in JSON format.
    
    Args:
        contract: Contract data in JSON format
    
    Returns:
        Processed contract information
    """
    try:
        result = await pipeline.process_document(contract.dict())
        return result
    except Exception as e:
        logger.error(f"Error ingesting contract: {e}")
        raise HTTPException(status_code=500, detail=f"Error ingesting contract: {str(e)}")

@router.post("/contracts/ingest-pdf", response_model=Dict[str, Any])
async def ingest_pdf_contract(
    file: UploadFile = File(...),
    title: Optional[str] = Form(None),
    contract_type: Optional[str] = Form(None),
    tags: Optional[str] = Form(None)
):
    """
    Ingest a contract from a PDF file.
    
    Args:
        file: PDF file
        title: Optional title override
        contract_type: Optional contract type
        tags: Optional comma-separated tags
    
    Returns:
        Processed contract information
    """
    if not file.filename.lower().endswith('.pdf'):
        raise HTTPException(status_code=400, detail="File must be a PDF")
    
    # Save uploaded file temporarily
    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.pdf')
    try:
        # Write file content
        content = await file.read()
        temp_file.write(content)
        temp_file.close()
        
        # Process PDF
        contract_data = await pdf_processor.process_pdf(temp_file.name)
        
        # Override with provided form data if any
        if title:
            contract_data["title"] = title
        if contract_type:
            contract_data["metadata"]["contract_type"] = contract_type
        if tags:
            contract_data["metadata"]["tags"] = [tag.strip() for tag in tags.split(',')]
        
        # Validate contract structure
        try:
            contract_obj = ContractCreate.parse_obj(contract_data)
        except ValidationError as ve:
            logger.error(f"Invalid contract structure: {ve}")
            raise HTTPException(status_code=400, detail=f"Invalid contract structure: {ve}")
        
        # Process through pipeline
        result = await pipeline.process_document(contract.dict())
        return result
        
    except Exception as e:
        logger.error(f"Error processing PDF contract: {e}")
        raise HTTPException(status_code=500, detail=f"Error processing PDF: {str(e)}")
    finally:
        # Clean up temporary file
        if os.path.exists(temp_file.name):
            os.unlink(temp_file.name)

@router.get("/contracts/{contract_id}", response_model=Dict[str, Any])
async def get_contract(contract_id: str):
    """
    Get contract by ID.
    
    Args:
        contract_id: Contract ID
    
    Returns:
        Contract data
    """
    try:
        # Assuming pipeline or a repository has this method
        contract = await pipeline.get_contract(contract_id)
        if not contract:
            raise HTTPException(status_code=404, detail="Contract not found")
        return contract
    except Exception as e:
        logger.error(f"Error getting contract: {e}")
        raise HTTPException(status_code=500, detail=f"Error retrieving contract: {str(e)}")

@router.get("/contracts", response_model=List[Dict[str, Any]])
async def list_contracts(
    limit: int = Query(100, ge=1, le=1000),
    skip: int = Query(0, ge=0),
    contract_type: Optional[str] = None,
    status: Optional[str] = None
):
    """
    List contracts with optional filtering.
    
    Args:
        limit: Maximum number of contracts to return
        skip: Number of contracts to skip
        contract_type: Filter by contract type
        status: Filter by status
    
    Returns:
        List of contracts
    """
    try:
        # Build filters
        filters = {}
        if contract_type:
            filters["metadata.contract_type"] = contract_type
        if status:
            filters["metadata.status"] = status
            
        # Assuming pipeline or a repository has this method
        contracts = await pipeline.get_contracts(filters, limit, skip)
        return contracts
    except Exception as e:
        logger.error(f"Error listing contracts: {e}")
        raise HTTPException(status_code=500, detail=f"Error listing contracts: {str(e)}")

@router.post("/search", response_model=Dict[str, Any])
async def search_contracts(
    query: str,
    filters: Optional[Dict[str, Any]] = None,
    top_k: int = Query(5, ge=1, le=100)
):
    """
    Search contracts using the query router.
    
    Args:
        query: Search query
        filters: Optional filters
        top_k: Number of results to return
    
    Returns:
        Search results
    """
    try:
        results = await query_router.route_query(query, filters, top_k)
        return results
    except Exception as e:
        logger.error(f"Error performing search: {e}")
        raise HTTPException(status_code=500, detail=f"Error performing search: {str(e)}")

=== End of ./api/routes.py ===


=== File: ./api/__init__.py ===


=== End of ./api/__init__.py ===


=== File: ./cli.py ===
#!/usr/bin/env python
"""
Command-line interface for CLM Smart Search System.
Allows users to ingest contracts, search, and manage the system.
"""
import asyncio
import json
import logging
import os
import functools

import click
from dotenv import load_dotenv
from pydantic import ValidationError

from clm_system.schemas.schemas import ContractCreate, EmailCreate, DealCreate, RecapCreate
from clm_system.core.pipeline.orchestrator import PipelineService
from clm_system.core.query_engine.search import QueryRouter
from clm_system.core.utils.embeddings import get_embedding_model, compute_embedding
from clm_system.core.pipeline.preprocessing.pdf_processor import PDFProcessor
# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=getattr(logging, os.getenv("LOG_LEVEL", "INFO")),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger("clm-cli")

# Initialize services
pipeline = PipelineService()      # no args, uses its internal defaults
query_router = QueryRouter()

def async_command(func):
    """Decorator to run async commands."""
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        return asyncio.run(func(*args, **kwargs))
    return wrapper

def load_file(file_path):
    """Load content from a file."""
    try:
        return json.load(open(file_path, 'r'))
    except Exception as e:
        click.echo(f"Failed to read file {file_path}: {e}", err=True)
        return None

def detect_document_type(file_path):
    """Detect document type from file extension or content."""
    # Simple extension-based detection for demo
    if file_path.lower().endswith('.contract.json'):
        return 'contract'
    elif file_path.lower().endswith('.email.json'):
        return 'email'
    elif file_path.lower().endswith('.deal.json'):
        return 'deal'
    elif file_path.lower().endswith('.recap.json'):
        return 'recap'
    else:
        # Default to contract for now
        return 'contract'

def handle_validation_error(error):
    """Handle and display validation errors."""
    click.echo("Invalid document JSON:", err=True)
    click.echo(error.json(), err=True)

def show_ingestion_result(doc_type, result):
    """Display ingestion result based on document type."""
    click.echo(f"{doc_type.capitalize()} ingested successfully: {result['id']}")
    click.echo(f"Title: {result['title']}")
    
    if doc_type == 'contract' and 'clauses_count' in result:
        click.echo(f"Clauses: {result['clauses_count']}")
    elif doc_type == 'email' and 'has_attachments' in result:
        click.echo(f"Has attachments: {result['has_attachments']}")
    elif doc_type == 'deal' and 'deal_type' in result:
        click.echo(f"Deal type: {result['deal_type']}")
    elif doc_type == 'recap' and 'participants' in result:
        click.echo(f"Participants: {len(result.get('participants', []))}")
        
    click.echo(f"Status: {result['status']}")

@click.group()
def cli():
    """CLM Smart Search System CLI"""
    pass

@cli.command()
@click.argument('file_path', type=click.Path(exists=True))
@click.option('--type', '-t', type=click.Choice(['contract', 'email', 'recap', 'deal']),
              help='Explicit document type')
@async_command
async def ingest(file_path, type):
    """Ingest any supported document type into the system."""
    raw = load_file(file_path)
    if not raw:
        return
    
    # Map file extensions to types if not specified
    if not type:
        type = detect_document_type(file_path)
        
    try:
        # Validate against appropriate schema
        if type == 'contract':
            doc = ContractCreate.parse_obj(raw)
        elif type == 'email':
            doc = EmailCreate.parse_obj(raw)
        elif type == 'recap':
            doc = RecapCreate.parse_obj(raw)
        elif type == 'deal':
            doc = DealCreate.parse_obj(raw)
            
        result = await pipeline.process_document(doc.dict())
        show_ingestion_result(type, result)
        
    except ValidationError as e:
        handle_validation_error(e)

@cli.command()
@click.argument('query')
@click.option('--filters', '-f', help='JSON string with filters')
@click.option('--type', '-t', type=click.Choice(['contract', 'email', 'recap', 'deal']),
              help='Limit search to specific document type')
@click.option('--top-k', '-k', type=int, default=5, help='Number of results to return')
@async_command
async def search(query, filters=None, type=None, top_k=5):
    """Search for documents using the query router."""
    # Parse filters JSON if provided
    try:
        filter_dict = json.loads(filters) if filters else {}
    except Exception as e:
        click.echo(f"Invalid filters JSON: {e}", err=True)
        return

    # Add document_type filter if specified
    if type:
        filter_dict["metadata.document_type"] = type

    # Perform search
    try:
        results = await query_router.route_query(query, filter_dict, top_k)
    except Exception as e:
        click.echo(f"Error performing search: {e}", err=True)
        return

    # Display results
    click.echo(f"Query: {results['query']}")
    click.echo(f"Query type: {results['metadata']['query_type']}")
    click.echo(f"Total results: {results['total_results']}")
    click.echo(f"Execution time: {results['execution_time_ms']:.2f}ms")
    click.echo("\nResults:")
    for i, r in enumerate(results['results'], 1):
        click.echo(f"\n--- Result {i} ---")
        doc_type = r.get('metadata', {}).get('document_type', 'document')
        click.echo(f"{doc_type.capitalize()}: {r.get('document_title', 'Untitled')} (ID: {r.get('document_id', 'Unknown')})")
        if 'clause_title' in r or 'clause_type' in r:
            click.echo(f"Clause: {r.get('clause_title', r.get('clause_type', 'Unknown'))}")
        click.echo(f"Relevance: {r['relevance_score']:.4f}")
        click.echo(f"Content: {r['content'][:100]}...")

@cli.command()
@click.option('--model-name', '-m', help='Override the default embedding model')
def test_embedding(model_name=None):
    """Test the embedding model with a sample text."""
    try:
        if model_name:
            os.environ["EMBEDDING_MODEL"] = model_name
            click.echo(f"Using model: {model_name}")
        else:
            click.echo(f"Using default model: {os.getenv('EMBEDDING_MODEL')}")

        model = get_embedding_model()
        sample_text = "This is a test sentence to verify the embedding model is working correctly."
        embedding = compute_embedding(sample_text, model)

        click.echo(f"Successfully computed embedding with dimensions: {len(embedding)}")
        click.echo(f"First 5 values: {embedding[:5]}")
    except Exception as e:
        click.echo(f"Error testing embedding model: {e}", err=True)

@cli.command()
@click.argument('pdf_path', type=click.Path(exists=True))
@click.option('--title', '-t', help='Optional title override')
@click.option('--type', '-d', type=click.Choice(['contract', 'email', 'recap', 'deal']), 
              default='contract', help='Document type')
@click.option('--document-type', '-dt', help='Specific document sub-type (e.g., NDA, lease)')
@click.option('--tags', help='Optional comma-separated tags')
@async_command
async def ingest_pdf(pdf_path, title=None, type='contract', document_type=None, tags=None):
    """Ingest a document from a PDF file."""
    try:
        if not pdf_path.lower().endswith('.pdf'):
            click.echo("File must be a PDF", err=True)
            return
            
        # Initialize PDF processor
        pdf_processor = PDFProcessor()
        
        # Process PDF
        click.echo(f"Processing PDF as {type}: {pdf_path}")
        document_data = await pdf_processor.process_pdf(pdf_path, doc_type=type)
        
        # Override with provided options if any
        if title:
            document_data["title"] = title
        if document_type:
            if type == 'contract':
                document_data["metadata"]["contract_type"] = document_type
            elif type == 'deal':
                document_data["metadata"]["deal_type"] = document_type
        if tags:
            document_data["metadata"]["tags"] = [tag.strip() for tag in tags.split(',')]
        
        # Add document_type to metadata if not already present
        if "metadata" in document_data and "document_type" not in document_data["metadata"]:
            document_data["metadata"]["document_type"] = type
        
        # Validate document structure based on type
        try:
            if type == 'contract':
                doc_obj = ContractCreate.parse_obj(document_data)
            elif type == 'email':
                doc_obj = EmailCreate.parse_obj(document_data)
            elif type == 'recap':
                doc_obj = RecapCreate.parse_obj(document_data)
            elif type == 'deal':
                doc_obj = DealCreate.parse_obj(document_data)
        except ValidationError as ve:
            click.echo(f"Invalid {type} structure:", err=True)
            click.echo(ve.json(), err=True)
            return
        
        # Process through pipeline
        result = await pipeline.process_document(doc_obj.dict())
        
        # Success output
        show_ingestion_result(type, result)
        
    except Exception as e:
        click.echo(f"Error processing PDF document: {e}", err=True)

if __name__ == "__main__":
    cli()

=== End of ./cli.py ===


=== File: ./combiner.py ===
# combiner.py
import os
from pathlib import Path

# List of files to combine (relative to project root)
FILES_TO_COMBINE = [
    "clm_system/cli.py",
    "clm_system/schemas/schemas.py",
    "clm_system/core/query_engine/search.py",
    "clm_system/core/database/elasticsearch_client.py",
    "clm_system/core/pipeline/orchestrator.py",
    "clm_system/core/query_engine/query_classifier.py",
    # Add new files using the same pattern
]

def combine_files():
    project_root = Path(__file__).parent.parent  # Adjust based on your structure
    combined_path = project_root / "combined.txt"
    
    with open(combined_path, "w", encoding="utf-8") as combined_file:
        for rel_path in FILES_TO_COMBINE:
            abs_path = project_root / rel_path
            header = f"\n\n{'=' * 80}\n# FILE: {rel_path}\n{'=' * 80}\n\n"
            
            try:
                combined_file.write(header)
                with open(abs_path, "r", encoding="utf-8") as source_file:
                    combined_file.write(source_file.read())
                print(f"✓ Added {rel_path}")
            except FileNotFoundError:
                print(f"⚠️  Missing: {abs_path}")
            except Exception as e:
                print(f"⚠️  Error reading {rel_path}: {str(e)}")

    print(f"\nCombined file created at: {combined_path}")

if __name__ == "__main__":
    print("Starting file combination...")
    combine_files()

=== End of ./combiner.py ===


=== File: ./config.py ===
# File: clm_system/config.py
import os
from functools import lru_cache
from typing import Optional

from pydantic import BaseSettings, Field
from dotenv import load_dotenv
load_dotenv()

class Settings(BaseSettings):
    
    openai_api_key: str = os.getenv("OPENAI_API_KEY", "")
    embedding_model: str = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
    chunk_size: int = int(os.getenv("CHUNK_SIZE", "512"))
    chunk_overlap: int = int(os.getenv("CHUNK_OVERLAP", "128"))
    default_top_k: int = int(os.getenv("DEFAULT_TOP_K", "5"))
    classifier_cache_ttl: int = int(os.getenv("CLASSIFIER_CACHE_TTL", "3600"))
    
    # MongoDB settings
    mongodb_uri: str = Field(..., env="MONGODB_URI")
    mongodb_database: str = Field("clm_db", env="MONGODB_DATABASE")
    
    # Elasticsearch settings
    elasticsearch_uri: str = Field(..., env="ELASTICSEARCH_URI")
    
    # Qdrant settings
    qdrant_uri: str = Field(..., env="QDRANT_URI")
    
    # API settings
    api_host: str = Field("0.0.0.0", env="API_HOST")
    api_port: int = Field(8000, env="API_PORT")
    
    # Embedding model
    embedding_model: str = Field(
        "sentence-transformers/all-MiniLM-L6-v2", env="EMBEDDING_MODEL"
    )
    
    # Default chunk size for text splitting
    chunk_size: int = Field(500, env="CHUNK_SIZE")
    chunk_overlap: int = Field(50, env="CHUNK_OVERLAP")
    
    # Vector settings
    vector_dimension: int = Field(384, env="VECTOR_DIMENSION")  # Default for MiniLM-L6
    
    # Search settings
    default_top_k: int = Field(5, env="DEFAULT_TOP_K")
    
    class Config:
        env_file = ".env"
        case_sensitive = False
    

@lru_cache()
def get_settings() -> Settings:
    return Settings()


settings = get_settings()



=== End of ./config.py ===


=== File: ./core/database/elasticsearch_client.py ===
# File: clm_system/core/database/elasticsearch_client.py
import logging
from typing import Dict, List, Any, Optional

from elasticsearch import AsyncElasticsearch, NotFoundError

from clm_system.config import settings

logger = logging.getLogger(__name__)

class ElasticsearchClient:
    """Client for interacting with Elasticsearch."""
    
    def __init__(self):
        self.client = AsyncElasticsearch(settings.elasticsearch_uri)
        self.index_name = "documents"  # Changed from 'contracts' to 'documents'
    
    async def ensure_index(self):
        """Ensures the documents index exists with proper mappings for all document types."""
        try:
            # Check if index exists
            exists = await self.client.indices.exists(index=self.index_name)
            if not exists:
                # Create index with mappings for all document types
                mappings = {
                    "mappings": {
                        "dynamic": "strict",
                        "properties": {
                            "id": {"type": "keyword"},
                            "title": {"type": "text", "analyzer": "standard"},
                            "created_at": {"type": "date"},
                            "updated_at": {"type": "date"},
                            "metadata": {
                                "properties": {
                                    "document_type": {"type": "keyword"},
                                    "tags": {"type": "keyword"},
                                    "status": {"type": "keyword"},
                                    "contract_type": {"type": "keyword"},
                                    "deal_type": {"type": "keyword"},
                                    "effective_date": {"type": "date"},
                                    "expiration_date": {"type": "date"},
                                    "parties": {
                                        "type": "nested",
                                        "properties": {
                                            "name": {"type": "text", "analyzer": "standard"},
                                            "id": {"type": "keyword"}
                                        }
                                    },
                                    "from_address": {"type": "keyword"},
                                    "to": {"type": "keyword"},
                                    "cc": {"type": "keyword"},
                                    "bcc": {"type": "keyword"},
                                    "subject": {"type": "text"},
                                    "has_attachments": {"type": "boolean"},
                                    "attachment_count": {"type": "integer"},  # Add this line
                                    "attachment_count": {"type": "integer"},  # Add this line
                                    "attachment_names": {"type": "keyword"},  # Add this line
                                    "sent_date": {"type": "date"},
                                    "volume": {"type": "keyword"},
                                    "price_per_unit": {"type": "float"},
                                    "meeting_date": {"type": "date"},
                                    "participants": {"type": "keyword"},
                                    "decisions": {"type": "text"},
                                    "action_items": {"type": "text"}
                                }
                            },  # Added missing comma here
                            # Content fields for different types
                            "content": {"type": "text"},  # Email content
                            "summary": {"type": "text"},  # Recap summary
                            "key_points": {"type": "text"},  # Recap key points
                            "financial_terms": {"type": "object"},  # Deal terms
                            "clauses": {
                                "type": "nested",
                                "properties": {
                                    "id": {"type": "keyword"},
                                    "title": {"type": "text", "analyzer": "standard"},
                                    "type": {"type": "keyword"},
                                    "text": {"type": "text", "analyzer": "standard"},
                                    "position": {"type": "integer"},
                                    "metadata": {
                                        "type": "object",
                                        "properties": {
                                            "attachment_names": {"type": "keyword"},  # Add this line
                                            "section_type": {"type": "keyword"}
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
                await self.client.indices.create(index=self.index_name, body=mappings)
                logger.info(f"Created Elasticsearch index {self.index_name}")
        except Exception as e:
            logger.error(f"Error ensuring Elasticsearch index: {str(e)}")
            raise
    
    async def index_document(self, document: Dict[str, Any]) -> Dict[str, Any]:
        """
        Indexes a document in Elasticsearch.
        
        Args:
            document: Document data (any type)
            
        Returns:
            Elasticsearch response
        """
        try:
            # Ensure index exists
            await self.ensure_index()
            
            # Index document
            response = await self.client.index(
                index=self.index_name,
                id=document["id"],
                document=document,
                refresh=True  # Make document immediately searchable
            )
            return response
        except Exception as e:
            logger.error(f"Elasticsearch index error: {str(e)}")
            raise
    
    # Alias for backward compatibility
    async def index_contract(self, contract: Dict[str, Any]) -> Dict[str, Any]:
        """Alias for index_document for backward compatibility."""
        return await self.index_document(contract)
    
    async def search(
        self, query: str, filters: Optional[Dict[str, Any]] = None, top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """
        Searches for documents in Elasticsearch.
        
        Args:
            query: Search query
            filters: Query filters
            top_k: Maximum number of results to return
            
        Returns:
            List of search results
        """
        try:
            await self.ensure_index()
            
            # Determine document type for type-specific field mapping
            doc_type = filters.get("metadata.document_type", "contract") if filters else "contract"
            
            # Base query structure
            search_query = {
                "query": {
                    "bool": {
                        "must": []
                    }
                },
                "size": top_k
            }
            
            # Add type-specific queries
            if doc_type == "contract" or doc_type == "deal":
                # For contract/deal, search in clauses and title
                search_query["query"]["bool"]["must"] = [
                    {
                        "nested": {
                            "path": "clauses",
                            "query": {
                                "multi_match": {
                                    "query": query,
                                    "fields": ["clauses.title^1.5", "clauses.text"]
                                }
                            },
                            "inner_hits": {
                                "highlight": {
                                    "fields": {
                                        "clauses.text": {"fragment_size": 150}
                                    }
                                },
                                "_source": True,
                                "size": 3  # Maximum number of clause matches per contract
                            }
                        }
                    },
                    {
                        "multi_match": {
                            "query": query,
                            "fields": ["title^2"]
                        }
                    }
                ]
            elif doc_type == "email":
                # For email, search in content and subject
                search_query["query"]["bool"]["must"] = [
                    {
                        "multi_match": {
                            "query": query,
                            "fields": ["content", "metadata.subject^2"]
                        }
                    }
                ]
            elif doc_type == "recap":
                # For recap, search in summary and key points
                search_query["query"]["bool"]["must"] = [
                    {
                        "multi_match": {
                            "query": query,
                            "fields": ["summary^2", "key_points"]
                        }
                    }
                ]

            # Add filters if provided
            if filters:
                filter_clauses = []
                for field, value in filters.items():
                    if field.startswith("metadata."):
                        filter_clauses.append({"term": {field: value}})
                    elif field.startswith("clauses."):
                        filter_clauses.append({
                            "nested": {
                                "path": "clauses",
                                "query": {"term": {field: value}}
                            }
                        })
                    else:
                        filter_clauses.append({"term": {field: value}})
                
                if filter_clauses:
                    search_query["query"]["bool"]["filter"] = filter_clauses

            response = await self.client.search(
                index=self.index_name,
                body=search_query
            )

            results = []
            for hit in response["hits"]["hits"]:
                source = hit["_source"]
                doc_type = source.get("metadata", {}).get("document_type", "contract")
                
                # Process results based on document type
                if doc_type in ["contract", "deal"]:
                    # Process contract/deal with clauses
                    inner_hits = hit.get("inner_hits", {}).get("clauses", {}).get("hits", {}).get("hits", [])
                    
                    for clause_hit in inner_hits:
                        clause_source = clause_hit["_source"]
                        highlights = clause_hit.get("highlight", {}).get("clauses.text", [])
                        highlight_text = highlights[0] if highlights else clause_source.get("text", "")[:150]
                        
                        results.append({
                            "clause_id": clause_source["id"],
                            "contract_id": source["id"],
                            "contract_title": source["title"],
                            "clause_type": clause_source["type"],
                            "clause_title": clause_source.get("title"),
                            "content": highlight_text,
                            "relevance_score": hit["_score"] * clause_hit["_score"],
                            "metadata": {
                                **source.get("metadata", {}),
                                **clause_source.get("metadata", {})
                            }
                        })
                elif doc_type == "email":
                    # Process email results
                    content_preview = source.get("content", "")[:150]
                    results.append({
                        "clause_id": f"{source['id']}_content",
                        "contract_id": source["id"],
                        "contract_title": source.get("metadata", {}).get("subject", "Email"),
                        "clause_type": "email_content",
                        "clause_title": None,
                        "content": content_preview,
                        "relevance_score": hit["_score"],
                        "metadata": source.get("metadata", {})
                    })
                elif doc_type == "recap":
                    # Process recap results
                    summary_preview = source.get("summary", "")[:150]
                    results.append({
                        "clause_id": f"{source['id']}_summary",
                        "contract_id": source["id"],
                        "contract_title": source.get("title", "Meeting Recap"),
                        "clause_type": "recap_summary",
                        "clause_title": None,
                        "content": summary_preview,
                        "relevance_score": hit["_score"],
                        "metadata": source.get("metadata", {})
                    })
            
            return sorted(results, key=lambda x: -x["relevance_score"])[:top_k]
        except Exception as e:
            logger.error(f"Elasticsearch search error: {str(e)}")
            raise

=== End of ./core/database/elasticsearch_client.py ===


=== File: ./core/database/mongodb_client.py ===
# clm_system/core/database/mongodb_client.py
import logging
from dateutil import parser
from typing import Any, Dict, List, Optional, Tuple  # Add Tuple to imports
from pymongo import MongoClient
from pymongo.errors import PyMongoError
from motor.motor_asyncio import AsyncIOMotorClient
from clm_system.config import settings

logger = logging.getLogger(__name__)

class MongoDBClient:
    def __init__(self):
        self.client = AsyncIOMotorClient(settings.mongodb_uri)
        self.db = self.client[settings.mongodb_database]
        self.documents_collection = self.db.documents  # Generic collection name
    
    async def insert_document(self, document: Dict[str, Any]) -> str:
        """
        Inserts any type of document into MongoDB.
        
        Args:
            document: Document data (contract, deal, email, etc.)
            
        Returns:
            ID of the inserted document
        """
        try:
            # Convert datetime to string for MongoDB
            doc = document.copy()
            self._normalize_datetimes(doc)
            
            # Validate required fields
            if "id" not in doc:
                raise ValueError("Document must contain an 'id' field")
            if "metadata" not in doc or "document_type" not in doc["metadata"]:
                raise ValueError("Document metadata must contain 'document_type'")
                
            result = await self.documents_collection.insert_one(doc)
            return str(result.inserted_id)
        except PyMongoError as e:
            logger.error(f"MongoDB insert error: {str(e)}")
            raise
        
    async def get_document(self, document_id: str) -> Optional[Dict[str, Any]]:
        """
        Retrieves any document by ID.
        
        Args:
            document_id: ID of the document to retrieve
            
        Returns:
            Document data or None if not found
        """
        try:
            document = await self.documents_collection.find_one({"id": document_id})
            return self._denormalize_datetimes(document)
        except PyMongoError as e:
            logger.error(f"MongoDB get error: {str(e)}")
            raise
    
    async def get_documents(
    self,
    filters: Optional[Dict[str, Any]] = None,
    limit: int = 100,
    skip: int = 0,
    sort: Optional[List[Tuple[str, int]]] = None  # Now properly typed
) -> List[Dict[str, Any]]:
        """
        Retrieves documents matching the given filters.
        
        Args:
            filters: Query filters
            limit: Maximum number of documents to return
            skip: Number of documents to skip
            sort: List of (field, direction) tuples to sort by
            
        Returns:
            List of documents
        """
        try:
            query = filters or {}
            cursor = self.documents_collection.find(query)
            
            if sort:
                cursor = cursor.sort(sort)
                
            documents = await cursor.skip(skip).limit(limit).to_list(length=None)
            return [self._denormalize_datetimes(doc) for doc in documents]
        except PyMongoError as e:
            logger.error(f"MongoDB get_documents error: {str(e)}")
            raise
    def _normalize_datetimes(self, document: Dict[str, Any]):
        """No longer convert datetime fields to strings"""
        pass

    def _denormalize_datetimes(self, document: Optional[Dict[str, Any]]):
        """No longer needed as dates are stored as Date objects"""
        return document

    # def _normalize_datetimes(self, document: Dict[str, Any]):
    #     """Convert datetime fields to ISO strings for MongoDB storage"""
    #     for field in ['created_at', 'updated_at']:
    #         if field in document:
    #             document[field] = document[field].isoformat()
                
    #     # Handle metadata dates
    #     if 'metadata' in document:
    #         for dt_field in ['effective_date', 'expiration_date', 'sent_date', 'meeting_date']:
    #             if dt_field in document['metadata'] and document['metadata'][dt_field] is not None:
    #                 document['metadata'][dt_field] = document['metadata'][dt_field].isoformat()

    # def _denormalize_datetimes(self, document: Optional[Dict[str, Any]]):
    #     """Convert ISO strings back to datetime objects"""
    #     if not document:
    #         return None
            
    #     for field in ['created_at', 'updated_at']:
    #         if field in document:
    #             document[field] = parser.isoparse(document[field])
                
    #     # Handle metadata dates
    #     if 'metadata' in document:
    #         for dt_field in ['effective_date', 'expiration_date', 'sent_date', 'meeting_date']:
    #             if dt_field in document['metadata'] and isinstance(document['metadata'][dt_field], str):
    #                 document['metadata'][dt_field] = parser.isoparse(document['metadata'][dt_field])
                    
    #     return document

=== End of ./core/database/mongodb_client.py ===


=== File: ./core/database/qdrant_client.py ===
# File: clm_system/core/database/qdrant_client.py
import logging
import uuid
from typing import Dict, List, Any, Optional

from qdrant_client import QdrantClient as QdrantClientLib, models
from clm_system.config import settings

logger = logging.getLogger(__name__)

class QdrantClient:
    """Client for interacting with Qdrant vector database with multi-document support"""
    
    def __init__(self):
        self.client = QdrantClientLib(url=settings.qdrant_uri)
        self.collection_name = "document_chunks"  # Generic collection name
        self.vector_size = settings.vector_dimension
    
    async def ensure_collection(self):
        """Ensures the vector collection exists with updated schema"""
        try:
            collections = self.client.get_collections().collections
            collection_names = [c.name for c in collections]
            
            if self.collection_name not in collection_names:
                self.client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=models.VectorParams(
                        size=self.vector_size,
                        distance=models.Distance.COSINE
                    )
                )

                # Create generic payload indexes
                index_config = [
                    ("document_id", models.PayloadSchemaType.KEYWORD),
                    ("chunk_type", models.PayloadSchemaType.KEYWORD),
                    ("document_type", models.PayloadSchemaType.KEYWORD),
                    ("tags", models.PayloadSchemaType.KEYWORD),
                    ("metadata.has_attachments", models.PayloadSchemaType.BOOL),  # Index for boolean metadata
                ]

                for field, schema_type in index_config:
                    self.client.create_payload_index(
                        collection_name=self.collection_name,
                        field_name=field,
                        field_schema=schema_type
                    )

                logger.info(f"Created Qdrant collection {self.collection_name}")
        except Exception as e:
            logger.error(f"Error ensuring collection: {str(e)}")
            raise

    async def store_embedding(
        self,
        document_id: str,
        document_title: str,
        chunk_id: str,
        chunk_type: str,
        content: str,
        embedding: List[float],
        document_type: str,
        chunk_title: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Stores any type of document chunk embedding
        """
        try:
            await self.ensure_collection()

            # Ensure metadata is a dictionary
            metadata_dict = metadata or {}
            
            # Ensure boolean values remain as booleans, not strings
            if "has_attachments" in metadata_dict:
                # Explicitly convert to boolean if it's not already
                if not isinstance(metadata_dict["has_attachments"], bool):
                    metadata_dict["has_attachments"] = bool(metadata_dict["has_attachments"])
                logger.debug(f"has_attachments is set to {metadata_dict['has_attachments']} ({type(metadata_dict['has_attachments']).__name__})")

            payload = {
                "document_id": document_id,
                "document_title": document_title,
                "chunk_id": chunk_id,
                "chunk_type": chunk_type,
                "document_type": document_type,
                "content": content,
                "metadata": metadata_dict,
            }

            if chunk_title:
                payload["chunk_title"] = chunk_title

            # Generate unique ID across all document types
            unique_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{document_id}_{chunk_id}"))

            # Log payload for debugging
            logger.debug(f"Storing point with payload: {payload}")

            self.client.upsert(
                collection_name=self.collection_name,
                points=[models.PointStruct(
                    id=unique_id,
                    vector=embedding,
                    payload=payload
                )]
            )
            return chunk_id
        except Exception as e:
            logger.error(f"Storage error: {str(e)}")
            raise

    async def search(
    self,
    embedding: List[float],
    filters: Optional[Dict[str, Any]] = None,
    top_k: int = 5
) -> List[Dict[str, Any]]:
        """Generic search across all document types with filters"""
        try:
            await self.ensure_collection()
            query_filter = self._build_filter(filters)

            response = self.client.search(
                collection_name=self.collection_name,
                query_vector=embedding,
                query_filter=query_filter,
                limit=top_k,
                with_payload=True
            )

            return [self._format_result(hit) for hit in response]
        except Exception as e:
            logger.error(f"Search error: {str(e)}")
            raise
    
    async def scroll(self, document_id: str) -> List[Dict[str, Any]]:
        """Helper method to verify points are stored in Qdrant"""
        try:
            scroll_filter = models.Filter(
                must=[models.FieldCondition(
                    key="document_id", 
                    match=models.MatchValue(value=document_id)
                )]
            )
            
            points, _ = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=scroll_filter,
                with_payload=True,
                with_vectors=False
            )
            
            # Log the first point's payload structure for debugging
            if points:
                logger.debug(f"Sample point payload structure: {points[0].payload}")
                # Check if has_attachments exists and log its type
                if 'metadata' in points[0].payload and 'has_attachments' in points[0].payload['metadata']:
                    val = points[0].payload['metadata']['has_attachments']
                    logger.debug(f"has_attachments value: {val}, type: {type(val).__name__}")
            
            return [self._format_result(point) for point in points]
        except Exception as e:
            logger.error(f"Scroll error: {str(e)}")
            raise
    
    async def debug_points(self, document_id: str):
        """Debug method to print detailed point information"""
        try:
            points, _ = self.client.scroll(
                collection_name=self.collection_name,
                scroll_filter=models.Filter(
                    must=[models.FieldCondition(
                        key="document_id", 
                        match=models.MatchValue(value=document_id)
                    )]
                ),
                limit=10,
                with_payload=True
            )
            
            debug_info = []
            for i, point in enumerate(points):
                debug_info.append(f"Point {i+1}:")
                debug_info.append(f"  ID: {point.id}")
                debug_info.append(f"  Payload: {point.payload}")
                
                # Check metadata structure
                if 'metadata' in point.payload:
                    metadata = point.payload['metadata']
                    debug_info.append(f"  Metadata:")
                    for key, value in metadata.items():
                        debug_info.append(f"    {key}: {value} (type: {type(value).__name__})")
            
            return "\n".join(debug_info)
        except Exception as e:
            logger.error(f"Debug error: {str(e)}")
            return f"Error debugging points: {str(e)}"
    
    def _format_result(self, hit) -> Dict[str, Any]:
        """Standardizes result format across document types"""
        payload = hit.payload
        return {
            "document_id": payload["document_id"],
            "document_title": payload["document_title"],
            "document_type": payload["document_type"],
            "chunk_id": payload["chunk_id"],
            "chunk_type": payload["chunk_type"],
            "content": payload["content"],
            "relevance_score": hit.score if hasattr(hit, "score") else 1.0,
            "metadata": payload.get("metadata", {}),
            "chunk_title": payload.get("chunk_title")
        }
    
    def _build_filter(self, filters: Optional[Dict[str, Any]] = None) -> Optional[models.Filter]:
        """Convert filters dictionary to Qdrant Filter object"""
        if not filters:
            return None

        conditions = []
        for field, value in filters.items():
            # Handle nested metadata filters
            if field.startswith("metadata."):
                key = field.split(".", 1)[1]
                # Ensure booleans are correctly typed
                if isinstance(value, bool) or value in (True, False):
                    conditions.append(
                        models.FieldCondition(
                            key=f"metadata.{key}",
                            match=models.MatchValue(value=bool(value))
                        )
                    )
                else:
                    conditions.append(
                        models.FieldCondition(
                            key=f"metadata.{key}",
                            match=models.MatchValue(value=value)
                        )
                    )
            else:
                conditions.append(
                    models.FieldCondition(
                        key=field,
                        match=models.MatchValue(value=value)
                    )
                )

        return models.Filter(must=conditions) if conditions else None

=== End of ./core/database/qdrant_client.py ===


=== File: ./core/database/__init__.py ===


=== End of ./core/database/__init__.py ===


=== File: ./core/pipeline/base.py ===
# clm_system/core/pipeline/base.py
from abc import ABC, abstractmethod
from typing import ClassVar, Dict, Type, Any, Optional

class BaseStep(ABC):
    """Abstract base class for all pipeline steps with auto-registration"""
    _registry: ClassVar[Dict[str, Dict[str, Type["BaseStep"]]]] = {
        'ingestor': {},
        'cleaner': {},
        'chunker': {}
    }
    _step_type: str  # Must be set by subclasses (ingestor, cleaner, chunker)

    def __init_subclass__(cls, *, doc_type: Optional[str] = None, **kwargs):
        super().__init_subclass__(**kwargs)
        
        # Skip direct subclasses of BaseStep (abstract base classes)
        if BaseStep in cls.__bases__:
            return
        
        # Check if the class is abstract (has any abstract methods)
        is_abstract = bool(getattr(cls, '__abstractmethods__', None))
        
        if doc_type is None:
            if not is_abstract:
                raise TypeError(f"Concrete {cls._step_type} subclasses must provide 'doc_type'")
            return
        
        registry = cls._registry[cls._step_type]
        if doc_type in registry:
            raise ValueError(f"Duplicate {cls._step_type} registration for doc_type: {doc_type}")
        registry[doc_type] = cls
        cls.doc_type = doc_type

    @abstractmethod
    def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Process the document data"""
        pass

class BaseIngestor(BaseStep, ABC):
    """Base class for document ingestion"""
    _step_type = 'ingestor'

class BaseCleaner(BaseStep, ABC):
    """Base class for document cleaning"""
    _step_type = 'cleaner'

class BaseChunker(BaseStep, ABC):
    """Base class for document chunking"""
    _step_type = 'chunker'
    
    @abstractmethod
    def chunk(self, text: str) -> list[str]:
        """Split text into chunks"""
        pass

=== End of ./core/pipeline/base.py ===


=== File: ./core/pipeline/chunking/base.py ===
# file: core/pipeline/chunking/base.py 
from abc import ABC, abstractmethod

class ChunkerABC(ABC):
    @abstractmethod
    def chunk(self, raw: dict) -> dict:
        pass


=== End of ./core/pipeline/chunking/base.py ===


=== File: ./core/pipeline/chunking/contract.py ===
# clm_system/core/pipeline/chunking/contract.py
from typing import List
from spacy.lang.en import English
import spacy
import logging
from ..base import BaseChunker
from clm_system.config import settings
from typing import Dict, Any
logger = logging.getLogger(__name__)

class ContractChunker(BaseChunker, doc_type="contract"):
    def __init__(self):
        self.nlp = English()
        self.nlp.add_pipe("sentencizer")
        logger.info("SpaCy pipeline components: %s", self.nlp.pipe_names)

    def chunk(self, text: str) -> List[str]:
        doc = self.nlp(text)
        sentences = [sent.text.strip() for sent in doc.sents]
        
        size = settings.chunk_size
        overlap = settings.chunk_overlap
        chunks = []
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            sentence_tokens = sentence.split()
            sentence_len = len(sentence_tokens)
            
            if sentence_len > size:
                for i in range(0, sentence_len, size - overlap):
                    chunk_tokens = sentence_tokens[i:min(i + size, sentence_len)]
                    chunks.append(" ".join(chunk_tokens))
            else:
                if current_length + sentence_len > size:
                    chunks.append(" ".join(current_chunk))
                    overlap_tokens = min(overlap, len(current_chunk))
                    current_chunk = current_chunk[-overlap_tokens:] if overlap_tokens > 0 else []
                    current_length = len(current_chunk)
                
                current_chunk.extend(sentence_tokens)
                current_length += sentence_len

        if current_chunk:
            chunks.append(" ".join(current_chunk))

        return chunks
    
    def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        return data  # Implement if additional processing is needed

=== End of ./core/pipeline/chunking/contract.py ===


=== File: ./core/pipeline/chunking/deal.py ===
# clm_system/core/pipeline/chunking/deal.py
from typing import List
import re
from ..base import BaseChunker
from clm_system.config import settings
from typing import Dict, Any
class DealChunker(BaseChunker, doc_type="deal"):
    """Chunker for oil industry deal documents."""
    
    def chunk(self, text: str) -> List[str]:
        """
        Split deal text into semantic chunks for embedding.
        
        Args:
            text: Input text to chunk
            
        Returns:
            List of text chunks
        """
        if not text:
            return []
            
        # Get configuration
        size = settings.chunk_size
        overlap = settings.chunk_overlap
        
        # Split on section boundaries first
        sections = self._split_sections(text)
        
        # Process each section
        chunks = []
        for section in sections:
            # If section is already small enough, keep as is
            if len(section.split()) <= size:
                chunks.append(section)
                continue
                
            # Otherwise, chunk using sliding window
            tokens = section.split()
            for i in range(0, len(tokens), size - overlap):
                chunk_tokens = tokens[i:min(i + size, len(tokens))]
                if chunk_tokens:
                    chunks.append(" ".join(chunk_tokens))
        
        # Ensure each chunk is non-empty and deduplicate
        return [chunk for chunk in chunks if chunk.strip()]
    
    def _split_sections(self, text: str) -> List[str]:
        """Split text into logical sections based on headers/markers."""
        # Common section markers in oil & gas deals
        section_markers = [
            r'DEAL SUMMARY',
            r'PRICING TERMS',
            r'VOLUME DETAILS',
            r'DELIVERY TERMS',
            r'QUALITY SPECIFICATIONS',
            r'PAYMENT TERMS',
            r'SPECIAL PROVISIONS',
            r'FORCE MAJEURE',
            r'REGULATORY COMPLIANCE',
        ]
        
        # Create a regex pattern matching any of the section markers
        pattern = '|'.join(f'({marker})' for marker in section_markers)
        
        # Find all matches
        matches = list(re.finditer(pattern, text, re.IGNORECASE))
        
        if not matches:
            # No section markers found, return whole text
            return [text]
        
        # Extract sections using the positions of the matches
        sections = []
        
        # First section (from start to first match)
        if matches[0].start() > 0:
            sections.append(text[:matches[0].start()].strip())
        
        # Middle sections
        for i in range(len(matches)):
            start = matches[i].start()
            end = matches[i+1].start() if i < len(matches) - 1 else len(text)
            section_text = text[start:end].strip()
            if section_text:
                sections.append(section_text)
        
        return sections
    def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        return data  # Implement if additional processing is needed

=== End of ./core/pipeline/chunking/deal.py ===


=== File: ./core/pipeline/chunking/email.py ===
# clm_system/core/pipeline/chunking/email.py
from typing import List
import re
from ..base import BaseChunker
from clm_system.config import settings
from typing import Dict, Any
class EmailChunker(BaseChunker, doc_type="email"):
    """Chunker for email documents."""
    
    def chunk(self, text: str) -> List[str]:
        """
        Split email text into semantic chunks for embedding.
        
        Args:
            text: Input text to chunk
            
        Returns:
            List of text chunks
        """
        if not text:
            return []
            
        # Get configuration
        size = settings.chunk_size
        overlap = settings.chunk_overlap
        
        # First identify logical parts (greeting, body, signature, etc.)
        parts = self._split_email_parts(text)
        
        # Process each part
        chunks = []
        for part in parts:
            # Short parts can be kept as-is
            if len(part.split()) <= size:
                chunks.append(part)
                continue
                
            # For longer parts, chunk with sliding window
            tokens = part.split()
            for i in range(0, len(tokens), size - overlap):
                chunk_tokens = tokens[i:min(i + size, len(tokens))]
                if chunk_tokens:  # Ensure non-empty
                    chunks.append(" ".join(chunk_tokens))
        
        # Ensure each chunk is non-empty and deduplicate
        return [chunk for chunk in chunks if chunk.strip()]
    
    def _split_email_parts(self, text: str) -> List[str]:
        """Split email into logical parts (greeting, body, signature, etc.)."""
        # Try to identify parts based on common patterns
        
        # Check for greeting
        greeting_patterns = [
            r'^(Dear\s+[^,\n]+[,\n])',
            r'^(Hi\s+[^,\n]+[,\n])',
            r'^(Hello\s+[^,\n]+[,\n])',
            r'^(Good\s+(morning|afternoon|evening)[^,\n]*[,\n])'
        ]
        
        # Check for signature
        signature_patterns = [
            r'(Best\s+regards,?\s*\n+.+)$',
            r'(Regards,?\s*\n+.+)$',
            r'(Thanks,?\s*\n+.+)$',
            r'(Thank\s+you,?\s*\n+.+)$',
            r'(Sincerely,?\s*\n+.+)$',
            r'(--\s*\n+.+)$'
        ]
        
        # Initialize parts
        parts = []
        remaining_text = text
        
        # Extract greeting
        for pattern in greeting_patterns:
            match = re.search(pattern, remaining_text, re.IGNORECASE | re.MULTILINE)
            if match:
                greeting = match.group(1).strip()
                if greeting:
                    parts.append(greeting)
                remaining_text = remaining_text[match.end():].strip()
                break
        
        # Extract signature
        signature = None
        for pattern in signature_patterns:
            match = re.search(pattern, remaining_text, re.IGNORECASE | re.MULTILINE | re.DOTALL)
            if match:
                signature = match.group(1).strip()
                remaining_text = remaining_text[:match.start()].strip()
                break
        
        # Process the body (remaining text)
        if remaining_text:
            # Try to split by paragraphs
            paragraphs = re.split(r'\n{2,}', remaining_text)
            
            # Add each significant paragraph as a separate part
            for paragraph in paragraphs:
                cleaned = paragraph.strip()
                if cleaned and len(cleaned) > 10:  # Avoid tiny fragments
                    parts.append(cleaned)
        
        # Add signature at the end if found
        if signature:
            parts.append(signature)
        
        # If no parts were identified, return original text
        if not parts:
            return [text]
            
        return parts
    def process(self, data: Dict[str, Any]) -> Dict[str, Any]:
        return data  # Implement if additional processing is needed

=== End of ./core/pipeline/chunking/email.py ===


=== File: ./core/pipeline/cleaning/base.py ===
# file: core/pipeline/cleaning/base.py 
from abc import ABC, abstractmethod

class CleanerABC(ABC):
    @abstractmethod
    def clean(self, raw: dict) -> dict:
        pass


=== End of ./core/pipeline/cleaning/base.py ===


=== File: ./core/pipeline/cleaning/contract.py ===
# clm_system/core/pipeline/cleaning/contract.py
from ..base import BaseCleaner

class ContractCleaner(BaseCleaner, doc_type="contract"):
    def process(self, data: dict) -> dict:
        # e.g. strip whitespace, normalize dates, remove PII
        # if already clean, return as‑is
        return data


=== End of ./core/pipeline/cleaning/contract.py ===


=== File: ./core/pipeline/cleaning/deal.py ===
# clm_system/core/pipeline/cleaning/deal.py
import re
from ..base import BaseCleaner

class DealCleaner(BaseCleaner, doc_type="deal"):
    def process(self, data: dict) -> dict:
        """
        Clean and normalize oil industry deal data.
        
        Args:
            data: Raw deal data
            
        Returns:
            Cleaned deal data
        """
        cleaned = data.copy()
        
        # Normalize deal types (lowercase, replace spaces with underscores)
        if "metadata" in cleaned and "deal_type" in cleaned["metadata"]:
            cleaned["metadata"]["deal_type"] = cleaned["metadata"]["deal_type"].lower().replace(" ", "_")
        
        # Standardize location formatting if present
        if "metadata" in cleaned and "location" in cleaned["metadata"]:
            cleaned["metadata"]["location"] = self._normalize_location(cleaned["metadata"]["location"])
            
        # Clean monetary values
        for field in ["price_per_unit", "total_value"]:
            if "metadata" in cleaned and field in cleaned["metadata"]:
                cleaned["metadata"][field] = self._normalize_monetary_value(cleaned["metadata"][field])
        
        # Clean volume 
        if "metadata" in cleaned and "volume" in cleaned["metadata"]:
            cleaned["metadata"]["volume"] = self._normalize_volume(cleaned["metadata"]["volume"])
                
        # Clean clauses text
        if "clauses" in cleaned:
            for clause in cleaned["clauses"]:
                if "text" in clause:
                    clause["text"] = self._clean_text(clause["text"])
        
        return cleaned
    
    def _normalize_location(self, location: str) -> str:
        """Normalize location strings."""
        if not location or not isinstance(location, str):
            return location
            
        # Convert to title case
        location = location.title()
        
        # Handle common abbreviations
        abbreviations = {
            " Usa": " USA",
            " Uk": " UK",
            " Uae": " UAE",
        }
        for abbr, replacement in abbreviations.items():
            location = location.replace(abbr, replacement)
            
        return location
    
    def _normalize_monetary_value(self, value):
        """Normalize monetary values to numeric format."""
        if isinstance(value, (int, float)):
            return value
            
        if not value or not isinstance(value, str):
            return value
            
        # Remove currency symbols and commas
        value = re.sub(r'[$£€,]', '', value)
        
        # Extract numeric part
        match = re.search(r'([\d.]+)', value)
        if match:
            try:
                return float(match.group(1))
            except ValueError:
                pass
                
        return value
    
    def _normalize_volume(self, volume):
        """Normalize volume values."""
        if isinstance(volume, (int, float)):
            return volume
            
        if not volume or not isinstance(volume, str):
            return volume
            
        # Remove commas, standardize units
        volume = re.sub(r'[,]', '', volume)
        
        # Match number and unit
        match = re.search(r'([\d.]+)\s*([a-zA-Z]+)', volume)
        if match:
            try:
                value = float(match.group(1))
                unit = match.group(2).lower()
                
                # Standardize units
                unit_mapping = {
                    'bbl': 'bbl',
                    'barrel': 'bbl',
                    'barrels': 'bbl',
                    'bbls': 'bbl',
                    'mcf': 'mcf',
                    'mmcf': 'mmcf',
                    'boe': 'boe',
                }
                
                if unit in unit_mapping:
                    return f"{value} {unit_mapping[unit]}"
                return volume
            except ValueError:
                pass
                
        return volume
    
    def _clean_text(self, text: str) -> str:
        """Clean text content."""
        if not text or not isinstance(text, str):
            return text
            
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Fix common OCR errors in oil & gas terminology
        corrections = {
            "0il": "Oil",
            "0ffshore": "Offshore",
            "C0ntract": "Contract",
            "Petr0leum": "Petroleum",
        }
        
        for error, correction in corrections.items():
            text = text.replace(error, correction)
            
        return text.strip()

=== End of ./core/pipeline/cleaning/deal.py ===


=== File: ./core/pipeline/cleaning/email.py ===
# clm_system/core/pipeline/cleaning/email.py
import re
from html import unescape
from bs4 import BeautifulSoup
from ..base import BaseCleaner

class EmailCleaner(BaseCleaner, doc_type="email"):
    def process(self, data: dict) -> dict:
        """
        Clean and normalize email data.
        
        Args:
            data: Raw email data
            
        Returns:
            Cleaned email data
        """
        cleaned = data.copy()
        
        # Clean email addresses
        for field in ["from", "to", "cc", "bcc"]:
            if "metadata" in cleaned and field in cleaned["metadata"]:
                if isinstance(cleaned["metadata"][field], list):
                    cleaned["metadata"][field] = [
                        self._normalize_email(email) for email in cleaned["metadata"][field]
                    ]
                else:
                    cleaned["metadata"][field] = self._normalize_email(cleaned["metadata"][field])
        
        # Clean title/subject
        if "title" in cleaned:
            cleaned["title"] = self._clean_subject(cleaned["title"])
            
        # Clean clauses text (usually email body)
        if "clauses" in cleaned:
            for clause in cleaned["clauses"]:
                if "text" in clause:
                    # Check if this is HTML content
                    if clause.get("metadata", {}).get("format") == "html" or \
                       (clause["text"].startswith("<") and ">" in clause["text"]):
                        clause["text"] = self._clean_html(clause["text"])
                    else:
                        clause["text"] = self._clean_text(clause["text"])
        
        return cleaned
    
    def _normalize_email(self, email_address):
        """Normalize email addresses."""
        if not email_address:
            return email_address
            
        if isinstance(email_address, str):
            # Convert to lowercase
            email_address = email_address.lower()
            
            # Extract actual email from "Name <email>" format
            match = re.search(r'<([^>]+)>', email_address)
            if match:
                email_address = match.group(1)
                
            # Remove whitespace
            email_address = email_address.strip()
            
        return email_address
    
    def _clean_subject(self, subject):
        """Clean email subject/title."""
        if not subject:
            return subject
            
        # Remove common prefixes
        prefixes = ["RE:", "FW:", "FWD:", "Re:", "Fw:", "Fwd:"]
        cleaned_subject = subject
        for prefix in prefixes:
            if cleaned_subject.startswith(prefix):
                cleaned_subject = cleaned_subject[len(prefix):].strip()
                
        # Remove excessive whitespace
        cleaned_subject = re.sub(r'\s+', ' ', cleaned_subject)
        
        return cleaned_subject.strip()
    
    def _clean_html(self, html_content):
        """Convert HTML content to plain text."""
        if not html_content:
            return ""
            
        try:
            # Parse HTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # Remove scripts, styles, and hidden divs
            for tag in soup(["script", "style", "meta", "head"]):
                tag.extract()
                
            # Get text content
            text = soup.get_text(separator=' ')
            
            # Remove excessive whitespace
            text = re.sub(r'\s+', ' ', text)
            
            # Replace HTML entities
            text = unescape(text)
            
            # Remove email signature markers and footers
            signature_markers = ["--", "Best regards", "Regards", "Thanks,", "Thank you,", "Sincerely,"]
            for marker in signature_markers:
                if marker in text:
                    # Find position and truncate (keeping some context)
                    pos = text.find(marker)
                    # Keep the marker but limit what comes after
                    signature_limit = min(pos + 100, len(text))
                    text = text[:signature_limit]
                    break
                    
            return text.strip()
        except Exception:
            # If HTML parsing fails, fall back to simple cleanup
            return self._clean_text(html_content)
    
    def _clean_text(self, text):
        """Clean plain text email content."""
        if not text:
            return ""
            
        # Remove excessive line breaks
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # Remove common email quotes (lines starting with >)
        lines = text.split('\n')
        cleaned_lines = [line for line in lines if not line.strip().startswith('>')]
        text = '\n'.join(cleaned_lines)
        
        # Remove common signature separators
        text = re.sub(r'-{2,}|_{2,}|={2,}', '', text)
        
        # Remove common email footers
        footer_patterns = [
            r'Sent from my iPhone',
            r'Sent from my mobile device',
            r'Get Outlook for',
            r'CONFIDENTIALITY NOTICE:',
            r'DISCLAIMER:',
            r'PRIVILEGED AND CONFIDENTIAL',
        ]
        
        for pattern in footer_patterns:
            match = re.search(pattern, text)
            if match:
                text = text[:match.start()]
                
        return text.strip()

=== End of ./core/pipeline/cleaning/email.py ===


=== File: ./core/pipeline/cleaning/recap.py ===
# clm_system/core/pipeline/cleaning/recap.py
import re
from ..base import BaseCleaner

class RecapCleaner(BaseCleaner, doc_type="recap"):
    def process(self, data: dict) -> dict:
        """
        Clean and normalize recap/summary data.
        
        Args:
            data: Raw recap data
            
        Returns:
            Cleaned recap data
        """
        cleaned = data.copy()
        
        # Normalize recap type
        if "metadata" in cleaned and "recap_type" in cleaned["metadata"]:
            cleaned["metadata"]["recap_type"] = cleaned["metadata"]["recap_type"].lower().replace(" ", "_")
        
        # Clean title
        if "title" in cleaned:
            cleaned["title"] = self._clean_title(cleaned["title"])
        
        # Clean participant names if present
        if "metadata" in cleaned and "participants" in cleaned["metadata"]:
            if isinstance(cleaned["metadata"]["participants"], list):
                cleaned["metadata"]["participants"] = [
                    self._clean_name(participant) for participant in cleaned["metadata"]["participants"]
                ]
        
        # Clean author name
        if "metadata" in cleaned and "author" in cleaned["metadata"]:
            cleaned["metadata"]["author"] = self._clean_name(cleaned["metadata"]["author"])
                
        # Clean clauses text
        if "clauses" in cleaned:
            for clause in cleaned["clauses"]:
                if "text" in clause:
                    clause["text"] = self._clean_text(clause["text"])
                if "title" in clause:
                    clause["title"] = self._clean_title(clause["title"])
        
        return cleaned
    
    def _clean_title(self, title):
        """Clean recap title."""
        if not title:
            return title
            
        # Remove unnecessary prefixes
        prefixes = ["RECAP:", "SUMMARY:", "MINUTES:", "Meeting:"]
        for prefix in prefixes:
            if title.startswith(prefix):
                title = title[len(prefix):].strip()
        
        # Remove excessive whitespace
        title = re.sub(r'\s+', ' ', title)
        
        # Ensure first letter is capitalized
        if title:
            title = title[0].upper() + title[1:]
            
        return title.strip()
    
    def _clean_name(self, name):
        """Clean person names."""
        if not name or not isinstance(name, str):
            return name
            
        # Remove titles
        titles = ["Mr.", "Mrs.", "Ms.", "Dr.", "Prof."]
        for title in titles:
            if name.startswith(title):
                name = name[len(title):].strip()
        
        # Remove parenthetical information (like departments)
        name = re.sub(r'\([^)]*\)', '', name)
        
        # Normalize capitalization
        name = name.title()
        
        return name.strip()
    
    def _clean_text(self, text):
        """Clean recap text content."""
        if not text or not isinstance(text, str):
            return text
            
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove page numbers and headers/footers
        text = re.sub(r'Page \d+ of \d+', '', text)
        text = re.sub(r'CONFIDENTIAL', '', text, flags=re.IGNORECASE)
        
        # Remove bullet point decorations but keep content
        text = re.sub(r'•\s*', '- ', text)
        text = re.sub(r'★\s*', '- ', text)
        text = re.sub(r'✓\s*', '- ', text)
        
        # Fix common OCR errors
        corrections = {
            "0ption": "Option",
            "decisíon": "decision",
            "actíon": "action",
            "revíew": "review",
        }
        
        for error, correction in corrections.items():
            text = text.replace(error, correction)
            
        return text.strip()

=== End of ./core/pipeline/cleaning/recap.py ===


=== File: ./core/pipeline/ingestion/base.py ===
# file: core/pipeline/ingestion/base.py 
from abc import ABC, abstractmethod

class IngestorABC(ABC):
    @abstractmethod
    def ingest(self, raw: dict) -> dict:
        pass


=== End of ./core/pipeline/ingestion/base.py ===


=== File: ./core/pipeline/ingestion/contract.py ===
# clm_system/core/pipeline/ingestion/contract.py
import uuid
from datetime import datetime
from ..base import BaseIngestor
from typing import Dict, Any
class ContractIngestor(BaseIngestor, doc_type="contract"):
    def process(self, raw: Dict[str, Any]) -> Dict[str, Any]:
        data = raw.copy()
        data.setdefault("id", str(uuid.uuid4()))
        now = datetime.utcnow()
        data["created_at"] = now
        data["updated_at"] = now
        return data

=== End of ./core/pipeline/ingestion/contract.py ===


=== File: ./core/pipeline/ingestion/deal.py ===
# clm_system/core/pipeline/ingestion/deal.py
import uuid
from datetime import datetime
from ..base import BaseIngestor

class DealIngestor(BaseIngestor, doc_type="deal"):
    def process(self, raw: dict) -> dict:
        """
        Process raw oil industry deal data into a standardized format.
        
        Args:
            raw: Raw deal data from input source
            
        Returns:
            Standardized deal document with metadata
        """
        data = raw.copy()
        
        # Ensure required fields
        data.setdefault("id", str(uuid.uuid4()))
        now = datetime.utcnow()
        data.setdefault("created_at", now)
        data.setdefault("updated_at", now)
        
        # Ensure deal-specific metadata
        if "metadata" not in data:
            data["metadata"] = {}
            
        # Set document type if not present
        data["metadata"].setdefault("document_type", "deal")
        
        # Ensure deal-specific fields
        data["metadata"].setdefault("deal_type", data.get("deal_type", "unspecified"))
        data["metadata"].setdefault("volume", data.get("volume"))
        data["metadata"].setdefault("price_per_unit", data.get("price_per_unit"))
        data["metadata"].setdefault("total_value", data.get("total_value"))
        data["metadata"].setdefault("location", data.get("location"))
        data["metadata"].setdefault("counterparties", data.get("counterparties", []))
        
        return data

=== End of ./core/pipeline/ingestion/deal.py ===


=== File: ./core/pipeline/ingestion/email.py ===
# clm_system/core/pipeline/ingestion/email.py
import uuid
from datetime import datetime
from email.utils import parsedate_to_datetime
from ..base import BaseIngestor

class EmailIngestor(BaseIngestor, doc_type="email"):
    def process(self, raw: dict) -> dict:
        """
        Process raw email data into a standardized format.
        
        Args:
            raw: Raw email data from input source
            
        Returns:
            Standardized email document with metadata
        """
        data = raw.copy()
        
        # Ensure required fields
        data.setdefault("id", str(uuid.uuid4()))
        now = datetime.utcnow()
        data.setdefault("created_at", now)
        data.setdefault("updated_at", now)
        
        # Set title to email subject if not specified
        if "subject" in data and "title" not in data:
            data["title"] = data["subject"]
        
        # Ensure metadata exists
        if "metadata" not in data:
            data["metadata"] = {}
        
        # Set document type
        data["metadata"]["document_type"] = "email"
        
        # Extract and normalize email metadata
        data["metadata"].setdefault("from", data.get("from"))
        data["metadata"].setdefault("to", data.get("to", []))
        data["metadata"].setdefault("cc", data.get("cc", []))
        data["metadata"].setdefault("bcc", data.get("bcc", []))
        
        # Parse email date if present
        if "date" in data and isinstance(data["date"], str):
            try:
                parsed_date = parsedate_to_datetime(data["date"])
                data["metadata"]["email_date"] = parsed_date
            except Exception:
                # If parsing fails, use ingestion date
                data["metadata"]["email_date"] = now
        
        # Handle email body
        if "body" in data:
            # Store original body in metadata
            data["metadata"]["original_format"] = "text" if isinstance(data["body"], str) else "html"
            
            # Convert body to clauses
            if "clauses" not in data:
                data["clauses"] = [{
                    "id": f"{data['id']}_body",
                    "type": "email_body",
                    "text": data["body"],
                    "position": 0
                }]
            
            # Remove body from top level after storing in clauses
            data.pop("body", None)
            
        # Handle attachments metadata
        if "attachments" in data:
            data["metadata"]["has_attachments"] = True
            data["metadata"]["attachment_count"] = len(data["attachments"])
            data["metadata"]["attachment_names"] = [
                att.get("filename", f"attachment_{i}") 
                for i, att in enumerate(data["attachments"])
            ]
            # Remove attachments from top level
            data.pop("attachments", None)
        else:
            data["metadata"]["has_attachments"] = False
            
        return data

=== End of ./core/pipeline/ingestion/email.py ===


=== File: ./core/pipeline/ingestion/recap.py ===
# clm_system/core/pipeline/ingestion/recap.py
import uuid
from datetime import datetime
from ..base import BaseIngestor

class RecapIngestor(BaseIngestor, doc_type="recap"):
    def process(self, raw: dict) -> dict:
        """
        Process raw recap/summary data into a standardized format.
        
        Args:
            raw: Raw recap data from input source
            
        Returns:
            Standardized recap document with metadata
        """
        data = raw.copy()
        
        # Ensure required fields
        data.setdefault("id", str(uuid.uuid4()))
        now = datetime.utcnow()
        data.setdefault("created_at", now)
        data.setdefault("updated_at", now)
        
        # Ensure metadata exists
        if "metadata" not in data:
            data["metadata"] = {}
            
        # Set document type
        data["metadata"]["document_type"] = "recap"
        
        # Extract recap-specific metadata
        data["metadata"].setdefault("recap_type", data.get("recap_type", "general"))
        data["metadata"].setdefault("meeting_date", data.get("meeting_date"))
        data["metadata"].setdefault("author", data.get("author"))
        data["metadata"].setdefault("participants", data.get("participants", []))
        data["metadata"].setdefault("related_documents", data.get("related_documents", []))
        
        # Handle content sections
        if "sections" in data and isinstance(data["sections"], list):
            # Convert sections to clauses if not already present
            if "clauses" not in data:
                data["clauses"] = []
                for i, section in enumerate(data["sections"]):
                    if isinstance(section, dict):
                        data["clauses"].append({
                            "id": f"{data['id']}_section_{i}",
                            "title": section.get("heading", f"Section {i+1}"),
                            "type": section.get("type", "recap_section"),
                            "text": section.get("content", ""),
                            "position": i,
                            "metadata": {
                                "section_type": section.get("type", "general")
                            }
                        })
            # Remove sections from top level after storing in clauses
            data.pop("sections", None)
            
        return data

=== End of ./core/pipeline/ingestion/recap.py ===


=== File: ./core/pipeline/orchestrator.py ===
# clm_system/core/pipeline/orchestrator.py
import asyncio
import logging
from typing import Dict, Any, List, Optional

from clm_system.core.database.mongodb_client import MongoDBClient
from clm_system.core.database.elasticsearch_client import ElasticsearchClient
from clm_system.core.database.qdrant_client import QdrantClient
from clm_system.core.utils.embeddings import get_embedding_model, compute_embedding

# Import base classes to ensure registration
from .base import BaseIngestor, BaseCleaner, BaseChunker  # Add this line

logger = logging.getLogger(__name__)

class PipelineService:
    def __init__(self):
        # Remove hardcoded component dictionaries
        self.mongo = MongoDBClient()
        self.es = ElasticsearchClient()
        self.qdrant = QdrantClient()
        self.embedding_model = get_embedding_model()

    async def process_document(self, raw: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generic document processing pipeline:
        1. Determine document type
        2. Run through registered components
        3. Store results
        """
        try:
            doc_type = raw.get("metadata", {}).get("document_type", "contract")
            
            # Validate document type has registered components
            if not self._validate_components(doc_type):
                raise ValueError(f"No processors registered for document type: {doc_type}")

            # 1. Ingestion
            ingestor = BaseIngestor._registry['ingestor'][doc_type]()
            normalized = ingestor.process(raw)

            # 2. Cleaning
            cleaner = BaseCleaner._registry['cleaner'][doc_type]()
            cleaned = cleaner.process(normalized)

            # 3. Chunking (if chunker exists for type)
            chunks = []
            if doc_type in BaseChunker._registry['chunker']:
                chunker = BaseChunker._registry['chunker'][doc_type]()
                chunks = self._generate_chunks(cleaned, chunker)

            # 4. Persistence
            await self._persist_document(cleaned)
            
            if chunks:
                await self._store_embeddings(cleaned, chunks)

            return self._format_result(cleaned, doc_type, chunks)
            
        except KeyError as e:
            raise ValueError(f"Missing processor for document type {doc_type}") from e
        except Exception as e:
            logger.error(f"Document processing failed: {str(e)}")
            raise
        finally:
            await self._cleanup_connections()

    def _validate_components(self, doc_type: str) -> bool:
        """Check required components exist for document type"""
        has_ingestor = doc_type in BaseIngestor._registry['ingestor']
        has_cleaner = doc_type in BaseCleaner._registry['cleaner']
        return has_ingestor and has_cleaner

    def _generate_chunks(self, document: Dict, chunker: BaseChunker) -> List:
        """Generic chunk generation across document types"""
        chunks = []
        content_sources = [
            ("clauses", "text"),
            ("sections", "content"),
            ("body", None),
            ("content", None)
        ]

        for source_field, text_field in content_sources:
            if source := document.get(source_field):
                for item in self._iter_content_items(source, text_field):
                    for chunk_text in chunker.chunk(item["text"]):
                        chunks.append({
                            "source": source_field,
                            "text": chunk_text,
                            "metadata": item.get("metadata", {})
                        })

        return chunks

    def _iter_content_items(self, source, text_field: Optional[str]):
        """Yield text content from various document structures"""
        if isinstance(source, list):
            for item in source:
                if text_field:
                    yield {"text": item.get(text_field, ""), "metadata": item.get("metadata", {})}
                else:
                    yield {"text": str(item), "metadata": {}}
        elif isinstance(source, dict):
            yield {"text": source.get(text_field, "") if text_field else str(source), "metadata": {}}
        else:
            yield {"text": str(source), "metadata": {}}

    async def _persist_document(self, document: Dict[str, Any]):
        """Store document in databases"""
        # MongoDB
        inserted_id = await self.mongo.insert_document(document)
        logger.debug(f"Inserted MongoDB document: {inserted_id}")
        
        # Elasticsearch
        es_response = await self.es.index_document(document)
        logger.debug(f"Elasticsearch response: {es_response}")

    async def _store_embeddings(self, document: Dict[str, Any], chunks: List):
        """Store chunks in vector database"""
        doc_id = document["id"]
        doc_title = document.get("title", "Untitled Document")
        doc_type = document["metadata"]["document_type"]
        doc_metadata = document.get("metadata", {})

        for chunk in chunks:
        
            combined_metadata = {
            **doc_metadata,
            **chunk.get("metadata", {})
            }   
            embedding = compute_embedding(chunk["text"], self.embedding_model)
            await self.qdrant.store_embedding(
                document_id=document["id"],
                document_title=document.get("title", "Untitled"),
                chunk_id=f"{document['id']}_{hash(chunk['text'])}",
                chunk_type=chunk["source"],
                content=chunk["text"],
                document_type=document["metadata"]["document_type"],
                metadata=combined_metadata,  # Now includes document-level fields
                embedding=compute_embedding(chunk["text"], self.embedding_model)
                )

    def _format_result(self, document: Dict, doc_type: str, chunks: List) -> Dict:
        """Create standardized result format"""
        result = {
            "id": document["id"],
            "title": document.get("title", f"Untitled {doc_type.title()}"),
            "document_type": doc_type,
            "status": "indexed",
            "chunks_processed": len(chunks)
        }

        # Add type-specific metadata
        metadata = document.get("metadata", {})
        result.update({
            k: metadata[k] 
            for k in ["contract_type", "deal_type", "has_attachments"]
            if k in metadata
        })

        return result

    async def _cleanup_connections(self):
        """Close database connections"""
        await self.es.client.close()
        self.qdrant.client.close()

=== End of ./core/pipeline/orchestrator.py ===


=== File: ./core/pipeline/preprocessing/pdf_processor.py ===
# clm_system/core/preprocessing/pdf_processor.py
import fitz  # PyMuPDF
import re
import logging
from typing import Dict, Any, List

logger = logging.getLogger(__name__)

class PDFProcessor:
    """Processes PDF files into contract JSON structure."""
    
    def __init__(self):
        # Initialize any needed resources
        pass
        
    async def process_pdf(self, file_path: str) -> Dict[str, Any]:
        """
        Process a PDF file into a contract structure.
        
        Args:
            file_path: Path to the PDF file
            
        Returns:
            Contract object in dictionary form
        """
        try:
            # Extract text from PDF
            text = await self._extract_text(file_path)
            logger.debug(f"Extracted raw text:\n{text[:1000]}...")  # First 1000 chars
            # Process the text into contract structure
            contract = await self._structure_contract(text, file_path)
            logger.debug("Structured contract:", contract)
            
            return contract
        except Exception as e:
            logger.error(f"Error processing PDF {file_path}: {e}")
            raise
    
    async def _extract_text(self, file_path: str) -> str:
        """Extract and normalize text from PDF."""
        text = ""
        try:
            doc = fitz.open(file_path)
            for page in doc:
                text += page.get_text("text", flags=fitz.TEXT_PRESERVE_LIGATURES)
            
            # Normalize text
            text = re.sub(r'\s+', ' ', text)  # Replace multiple whitespace
            text = re.sub(r'(\w)-\n(\w)', r'\1\2', text)  # Fix hyphenated words
            return text
        except Exception as e:
            logger.error(f"Error extracting text from PDF: {e}")
            raise
    
    async def _structure_contract(self, text: str, file_name: str) -> Dict[str, Any]:
        """
        Convert raw text into contract structure.
        Uses heuristics and patterns to identify contract components.
        """
        # Basic structure
        contract = {
            "title": self._extract_title(text, file_name),
            "metadata": self._extract_metadata(text),
            "clauses": self._extract_clauses(text)
        }
        logger.debug(f"Structured contract: {contract}")
        return contract
    
    def _extract_title(self, text: str, file_name: str) -> str:
        """Extract contract title from text or use filename."""
        # Look for title patterns (often in first few lines)
        first_lines = text.strip().split('\n')[:5]
        for line in first_lines:
            if re.search(r'agreement|contract|terms', line.lower()):
                return line.strip()
        
        # Fallback to filename
        base_name = os.path.basename(file_name)
        name_without_ext = os.path.splitext(base_name)[0]
        return name_without_ext.replace('_', ' ').title()
    
    def _extract_metadata(self, text: str) -> Dict[str, Any]:
        """Extract contract metadata."""
        metadata = {
            "contract_type": self._identify_contract_type(text),
            "parties": self._identify_parties(text),
            "status": "draft",  # Default status
            "tags": []  # Can be filled based on content analysis
        }
        
        # Extract dates
        dates = self._extract_dates(text)
        if dates.get("effective_date"):
            metadata["effective_date"] = dates["effective_date"]
        if dates.get("expiration_date"):
            metadata["expiration_date"] = dates["expiration_date"]
            
        return metadata
    
    def _identify_contract_type(self, text: str) -> str:
        """Identify contract type from text."""
        type_patterns = {
            "license": r'license|licensing|licensor|licensee',
            "service": r'service|services|provider|customer',
            "nda": r'confidential|non-disclosure|nda',
            "employment": r'employment|employer|employee|hire',
            "purchase": r'purchase|procurement|buyer|seller'
        }
        
        for contract_type, pattern in type_patterns.items():
            if re.search(pattern, text.lower()):
                return contract_type
                
        return "other"
    
    def _identify_parties(self, text: str) -> List[Dict[str, str]]:
        """Extract party information."""
        parties = []
        
        # Look for common party patterns
        party_patterns = [
            r'between\s+([^,]+),?\s+(?:a|an)\s+([^,]+)',
            r'(?:party|client|customer|vendor|supplier):\s*([^\n]+)',
            r'(?:hereinafter\s+referred\s+to\s+as\s+["\'])([^"\']+)'
        ]
        
        for pattern in party_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for i, match in enumerate(matches):
                party_name = match.group(1).strip()
                if party_name and len(party_name) < 100:  # Sanity check
                    parties.append({
                        "name": party_name,
                        "id": f"party-{len(parties)+1:03d}"
                    })
        
        return parties
    
    from dateutil import parser  # Add this import

    def _extract_dates(self, text: str) -> Dict[str, str]:
        dates = {}
        
        # Existing regex patterns
        effective_match = re.search(r'effective\s+date.*?(\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{2,4})', text, re.IGNORECASE)
        expiration_match = re.search(r'(?:expiration|termination|end)\s+date.*?(\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{2,4})', text, re.IGNORECASE)

        # Parse dates with flexible format handling
        try:
            if effective_match:
                dates["effective_date"] = parser.parse(effective_match.group(1)).isoformat()
        except:
            logger.warning(f"Could not parse effective date: {effective_match.group(1)}")

        try:
            if expiration_match:
                dates["expiration_date"] = parser.parse(expiration_match.group(1)).isoformat()
        except:
            logger.warning(f"Could not parse expiration date: {expiration_match.group(1)}")

        return dates
    
    def _extract_clauses(self, text: str) -> List[Dict[str, Any]]:
        """Extract clauses from contract text."""
        clauses = []
        logger.debug(f"Raw text for clause extraction:\n{text[:2000]}...")  # First 2000 chars
        
        # Enhanced section pattern with multiple variations
        section_pattern = r'''
            (?:\n|\r\n)  # Section starts with newline
            (?:          # Section number formats:
            (?:\d+[\.\)]?|          # 1, 2., 3)
            [A-Z][\.\)]|           # A., B)
            ARTICLE\s+[IVXLCDM]+|  # ARTICLE I
            SECTION\s+[\dA-Z]+|    # SECTION 1, SECTION A
            Clause\s+\d+|          # Clause 1
            §\s?[\dA-Z]+|          # §1, §A
            [IVXLCDM]+[\.\)]       # I., II)
            )
            )
            \s+          # Whitespace after number
            ([A-Z][^\.:\n]{5,})  # Title (capitalized, min 5 chars)
            [\.:]?       # Optional ending punctuation
            (?=\n|\r\n|$)  # Lookahead for newline or end
        '''
        
        sections = re.split(section_pattern, text, flags=re.IGNORECASE|re.VERBOSE)
        
        # Debug found sections
        logger.debug(f"Split sections: {sections[:10]}")  # First 10 sections
        
        position = 1
        for i in range(1, len(sections)-1, 2):
            title = sections[i].strip()
            content = sections[i+1].strip()
            
            if len(content) > 50:  # Increased minimum content length
                clause_type = self._identify_clause_type(title, content)
                clauses.append({
                    "id": f"clause-{position:03d}",
                    "title": title,
                    "type": clause_type,
                    "text": content,
                    "position": position
                })
                position += 1
                
        # Fallback: Split by paragraph if no sections found
        if not clauses:
            logger.warning("No sections found, falling back to paragraph split")
            paragraphs = [p.strip() for p in re.split(r'\n{2,}', text) if len(p.strip()) > 100]
            for i, para in enumerate(paragraphs):
                clauses.append({
                    "id": f"para-{i+1:03d}",
                    "title": f"Paragraph {i+1}",
                    "type": "uncategorized",
                    "text": para,
                    "position": i+1
                })
        
        logger.debug(f"Found {len(clauses)} clauses: {clauses}")
        logger.debug(f"Pre-filtered clauses: {len(clauses)} items")
        valid_clauses = [c for c in clauses if c.get("text")]
        logger.debug(f"Post-filtered clauses: {len(valid_clauses)} items")
                
        return [
        clause for clause in clauses
        if clause.get("text") and len(clause["text"]) > 50
    ]

=== End of ./core/pipeline/preprocessing/pdf_processor.py ===


=== File: ./core/pipeline/__init__.py ===
# Import all components to trigger registration



from .ingestion.contract import ContractIngestor
from .ingestion.email import EmailIngestor
from .ingestion.deal import DealIngestor
from .ingestion.recap import RecapIngestor

from .cleaning.contract import ContractCleaner
from .cleaning.email import EmailCleaner
from .cleaning.deal import DealCleaner
from .cleaning.recap import RecapCleaner

from .chunking.contract import ContractChunker
from .chunking.email import EmailChunker
from .chunking.deal import DealChunker

=== End of ./core/pipeline/__init__.py ===


=== File: ./core/query_engine/helpers.py ===
from typing import List, Dict 

def reciprocal_rank_fusion(
    results_a: List[Dict],
    results_b: List[Dict],
    k: int = 60,
    weight_a: float = 1.0,
    weight_b: float = 1.0
) -> List[Dict]:
    """
    Combines search results using Reciprocal Rank Fusion algorithm.
    """
    fused_results = {}
    
    # Process first result list
    for idx, item in enumerate(results_a):
        doc_id = f"{item['contract_id']}_{item['clause_id']}"
        rank = idx + 1
        score = weight_a * (1 / (k + rank))
        fused_results[doc_id] = {
            **item,
            "rrf_score": score,
            "origin": "elastic"
        }
    
    # Process second result list and update scores
    for idx, item in enumerate(results_b):
        doc_id = f"{item['contract_id']}_{item['clause_id']}"
        rank = idx + 1
        score = weight_b * (1 / (k + rank))
        
        if doc_id in fused_results:
            # Document exists in both lists, merge and update score
            fused_results[doc_id]["rrf_score"] += score
            fused_results[doc_id]["origin"] = "both"
            # Keep metadata from both if they differ
            if item.get("metadata") != fused_results[doc_id].get("metadata"):
                fused_results[doc_id]["metadata"] = {
                    **fused_results[doc_id].get("metadata", {}),
                    **item.get("metadata", {})
                }
        else:
            # New document, add to results
            fused_results[doc_id] = {
                **item,
                "rrf_score": score,
                "origin": "vector"
            }
    
    # Convert back to list and sort by RRF score
    combined = list(fused_results.values())
    sorted_results = sorted(combined, key=lambda x: -x["rrf_score"])
    
    # Normalize scores to 0-1 range
    if sorted_results:
        max_score = max(r["rrf_score"] for r in sorted_results)
        if max_score > 0:
            for r in sorted_results:
                r["relevance_score"] = r["rrf_score"] / max_score
                del r["rrf_score"]
                del r["origin"]  # Clean up temp field
    
    return sorted_results

=== End of ./core/query_engine/helpers.py ===


=== File: ./core/query_engine/query_classifier.py ===
import logging
import time
from typing import Optional
import asyncio
from openai import AsyncOpenAI, APIError, RateLimitError
from clm_system.config import settings

logger = logging.getLogger(__name__)

class QueryClassifier:
    def __init__(self):
        #self.client = AsyncOpenAI(api_key=settings.openai_api_key)
        self.client = AsyncOpenAI(
            base_url="http://localhost:1234/v1",  # Default LM Studio port
            api_key="deepseek-r1-distill-qwen-7b"  # Dummy key required by client
        )
        self.cache = {}  # Simple cache for demo purposes
        self.cache_ttl = 3600  # 1 hour TTL
        self.cache_timestamps = {}

    # Update the query_classifier.py file

    async def classify(self, query: str) -> dict:
        """
        Classifies the query type and detects document types mentioned.
        
        Returns:
            Dict with query_type and detected document types
        """
        # Check cache and TTL
        if query in self.cache:
            timestamp = self.cache_timestamps.get(query, 0)
            if time.time() - timestamp < self.cache_ttl:
                return self.cache[query]
        
        # Basic query type classification
        query_type = self._heuristic_classify(query)
        
        # Default all document types
        doc_types = self._detect_document_types(query)
        
        try:
            response = await self.client.chat.completions.create(
                model="local-model",
                messages=[{
                    "role": "system",
                    "content": """Classify legal document search queries. Respond with JSON:
                    {
                        "type": "structured|semantic|hybrid",
                        "doc_types": ["contract", "email", "deal", "recap"]  # Include only relevant types
                    }
                    
                    Detect document types from these patterns:
                    - Email: 'email', 'inbox', 'attachment', 'sent', 'received', 'message'
                    - Deal: 'deal', 'volume', 'price', 'barrel', 'lease', 'agreement'
                    - Recap: 'meeting', 'minutes', 'action items', 'decisions', 'recap'  
                    - Contract: 'contract', 'clause', 'nda', 'terms', 'provision'
                    """
                }, {
                    "role": "user",
                    "content": query
                }],
                temperature=0.1,
                max_tokens=100
            )

            try:
                # Try to parse as JSON
                import json
                result = json.loads(response.choices[0].message.content)
                
                if isinstance(result, dict):
                    if "type" in result:
                        query_type = result["type"] 
                    if "doc_types" in result and isinstance(result["doc_types"], list):
                        doc_types = result["doc_types"]
            except:
                # If JSON parsing fails, use fallback
                pass
            
            result = {
                "query_type": query_type,
                "doc_types": doc_types
            }
            
            # Update cache
            self.cache[query] = result
            self.cache_timestamps[query] = time.time()
            return result
                
        except Exception as e:
            logger.error(f"Classification failed: {str(e)}")
            return {
                "query_type": query_type,
                "doc_types": doc_types
            }

    def _detect_document_types(self, query: str) -> List[str]:
        """Detect document types mentioned in the query"""
        query = query.lower()
        types = []
        
        # Email markers
        if any(term in query for term in ['email', 'inbox', 'message', 'sent', 'received']):
            types.append('email')
            
        # Deal markers
        if any(term in query for term in ['deal', 'volume', 'price', 'barrel', 'lease']):
            types.append('deal')
            
        # Recap markers
        if any(term in query for term in ['meeting', 'minutes', 'recap', 'action item']):
            types.append('recap')
            
        # Contract markers
        if any(term in query for term in ['contract', 'clause', 'nda', 'agreement', 'provision']):
            types.append('contract')
            
        # Return all types if none detected
        if not types:
            return ['contract', 'email', 'deal', 'recap']
            
        return types

    def _heuristic_classify(self, query: str) -> str:
        """Fallback classification using heuristics."""
        structured_keywords = [
            "date:", "type:", "status:", "party:", "before:", "after:",
            "contract type", "effective date", "expiration date", "status is"
        ]
        
        has_structured = any(keyword in query.lower() for keyword in structured_keywords)
        
        if len(query.split()) <= 3 and not has_structured:
            return "semantic"
        
        if len(query.split()) > 3 and has_structured:
            return "hybrid"
        
        if has_structured:
            return "structured"
        
        return "semantic"

=== End of ./core/query_engine/query_classifier.py ===


=== File: ./core/query_engine/search.py ===
# File: clm_system/core/query_engine/search.py
import asyncio
import logging
import time
from typing import Dict, List, Optional, Any, Union

from clm_system.config import settings
from clm_system.core.database.elasticsearch_client import ElasticsearchClient
from clm_system.core.database.qdrant_client import QdrantClient
from clm_system.core.utils.embeddings import get_embedding_model, compute_embedding
from .query_classifier import QueryClassifier
from .helpers import reciprocal_rank_fusion  # Add this import

logger = logging.getLogger(__name__)

class QueryRouter:
    """
    Routes queries to either structured search (Elasticsearch) or 
    semantic search (Qdrant) based on query analysis.
    """
    
    def __init__(self):
        self.es_client = ElasticsearchClient()
        self.qdrant_client = QdrantClient()
        self.embedding_model = get_embedding_model()
        self.top_k = settings.default_top_k
        self.classifier = QueryClassifier()
    
    async def route_query(
        self, query: str, filters: Optional[Dict[str, Any]] = None, top_k: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        Analyzes the query and routes it to the appropriate search engine.
        
        Args:
            query: User's search query
            filters: Optional metadata filters
            top_k: Number of results to return
            
        Returns:
            Dict containing search results and metadata
        """
        start_time = time.time()
        
        if top_k is None:
            top_k = self.top_k
        
        # Determine query type using classifier
        classification = await self.classifier.classify(query)
        query_type = classification.get('query_type', 'semantic')
        logger.info(f"Query classified as {query_type}: {query}")
        
        # Convert document_type filter to list if needed
        if filters and 'metadata.document_type' in filters:
            doc_types = [filters['metadata.document_type']]
        else:
            doc_types = classification.get('doc_types', ['contract', 'email', 'recap', 'deal'])
        
        # Parallel searches per document type
        search_tasks = []
        for doc_type in doc_types:
            task = self._search_by_type(query, query_type, filters, top_k, doc_type)
            search_tasks.append(task)
        
        # Gather results from all searches
        type_results = await asyncio.gather(*search_tasks)
        
        # Merge results from different document types
        results = self._merge_results(type_results, top_k)
        
        execution_time = (time.time() - start_time) * 1000  # Convert to milliseconds
        
        return {
            "query": query,
            "total_results": len(results),
            "results": results,
            "metadata": {
                "query_type": query_type,
                "filters_applied": filters is not None,
                "document_types": doc_types
            },
            "execution_time_ms": execution_time
        }
    
    async def _search_by_type(self, query: str, query_type: str, 
                             filters: Optional[Dict[str, Any]], 
                             top_k: int, doc_type: str) -> List[Dict[str, Any]]:
        """
        Perform search for a specific document type.
        
        Args:
            query: Search query
            query_type: Classification of query (structured, semantic, hybrid)
            filters: Metadata filters
            top_k: Number of results to return
            doc_type: Document type to search for
            
        Returns:
            List of search results for this document type
        """
        # Add document type to filters
        type_filters = filters.copy() if filters else {}
        type_filters["metadata.document_type"] = doc_type
        
        if query_type == "structured":
            # Structured search using Elasticsearch
            results = await self.es_client.search(query, type_filters, top_k)
        elif query_type == "semantic":
            # Semantic search using Qdrant
            query_embedding = compute_embedding(query, self.embedding_model)
            results = await self.qdrant_client.search(query_embedding, type_filters, top_k)
        else:  # hybrid
            # Compute embedding here before passing to search
            query_embedding = compute_embedding(query, self.embedding_model)
            
            # Run searches in parallel
            es_results, qdrant_results = await asyncio.gather(
                self.es_client.search(query, type_filters, top_k * 2),
                self.qdrant_client.search(query_embedding, type_filters, top_k * 2)
            )
            
            # Combine results using RRF
            results = reciprocal_rank_fusion(
                es_results,
                qdrant_results,
                k=60,
                weight_a=0.4,  # Elasticsearch weight
                weight_b=0.6   # Vector search weight
            )[:top_k]
        
        # Add document type to results metadata
        for result in results:
            if "metadata" not in result:
                result["metadata"] = {}
            result["metadata"]["document_type"] = doc_type
        
        return results
    
    def _merge_results(self, type_results: List[List[Dict[str, Any]]], top_k: int) -> List[Dict[str, Any]]:
        """
        Merge results from different document types, sorting by relevance.
        
        Args:
            type_results: List of result lists from different document types
            top_k: Maximum number of results to return
            
        Returns:
            Combined and sorted list of results
        """
        # Flatten results from all document types
        all_results = []
        for results in type_results:
            all_results.extend(results)
        
        # Sort by relevance score
        sorted_results = sorted(all_results, key=lambda x: -x["relevance_score"])
        
        # Return top-k
        return sorted_results[:top_k]

=== End of ./core/query_engine/search.py ===


=== File: ./core/utils/embeddings.py ===


# File: clm_system/core/utils/embeddings.py
import logging
from typing import List, Optional

import torch
from sentence_transformers import SentenceTransformer

from clm_system.config import settings

logger = logging.getLogger(__name__)

# Global cache for embedding model
_embedding_model = None

def get_embedding_model() -> SentenceTransformer:
    """
    Gets or initializes the embedding model.
    
    Returns:
        SentenceTransformer model instance
    """
    global _embedding_model
    
    if _embedding_model is None:
        logger.info(f"Loading embedding model: {settings.embedding_model}")
        try:
            _embedding_model = SentenceTransformer(settings.embedding_model)
            logger.info(f"Embedding model loaded successfully")
        except Exception as e:
            logger.error(f"Error loading embedding model: {str(e)}")
            raise
    
    return _embedding_model

def compute_embedding(text: str, model: Optional[SentenceTransformer] = None) -> List[float]:
    """
    Computes embedding for a given text.
    
    Args:
        text: Input text to embed
        model: Optional pre-loaded model (if not provided, will get from cache)
        
    Returns:
        List of floats representing the text embedding
    """
    if model is None:
        model = get_embedding_model()
    
    try:
        # Compute embedding
        embedding = model.encode(text)
        
        # Convert to list if it's a tensor or numpy array
        if isinstance(embedding, torch.Tensor):
            embedding = embedding.tolist()
        elif hasattr(embedding, "tolist"):
            embedding = embedding.tolist()
        
        return embedding
    except Exception as e:
        logger.error(f"Error computing embedding: {str(e)}")
        raise

=== End of ./core/utils/embeddings.py ===


=== File: ./core/utils/__init__.py ===


=== End of ./core/utils/__init__.py ===


=== File: ./core/__init__.py ===


=== End of ./core/__init__.py ===


=== File: ./main.py ===
# File: clm_system/main.py
import logging
import os

import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from clm_system.api.routes import router as api_router
from clm_system.config import settings

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=getattr(logging, os.getenv("LOG_LEVEL", "INFO")),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(
    title="CLM Smart Search",
    description="Contract Lifecycle Management with Smart Search capabilities",
    version="0.1.0",
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # For production, specify exact origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include API routes
app.include_router(api_router, prefix="/api")

# Health check endpoint
@app.get("/health")
async def health_check():
    return {"status": "ok"}

if __name__ == "__main__":
    logger.info(f"Starting server on {settings.api_host}:{settings.api_port}")
    uvicorn.run(
        "clm_system.main:app",
        host=settings.api_host,
        port=settings.api_port,
        reload=True,
    )



=== End of ./main.py ===


=== File: ./schemas/schemas.py ===
# File: clm_system/api/schemas.py
from pydantic import BaseModel, Field, EmailStr
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
import uuid
from dateutil import parser

class BaseMetadata(BaseModel):
    document_type: str
    tags: List[str] = Field(default_factory=list)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

class ClauseBase(BaseModel):
    """Base schema for contract clauses."""
    id: Optional[str] = None
    title: Optional[str] = None
    type: str
    text: str
    position: Optional[int] = None
    metadata: Optional[Dict[str, Any]] = Field(default_factory=dict)

class ContractMetadata(BaseMetadata):
    document_type: str = "contract"
    contract_type: str
    effective_date: Optional[datetime] = None
    expiration_date: Optional[datetime] = None
    parties: List[Dict[str, str]] = Field(default_factory=list)
    status: Optional[str] = None

    @validator('effective_date', 'expiration_date', pre=True)
    def parse_dates(cls, value):
        if isinstance(value, str):
            try:
                return parser.parse(value)
            except:
                return None
        return value

class EmailMetadata(BaseMetadata):
    document_type: str = "email"
    from_address: EmailStr
    to: List[EmailStr]
    cc: List[EmailStr] = []
    bcc: List[EmailStr] = []
    subject: str
    has_attachments: bool = False
    sent_date: datetime

class DealMetadata(BaseMetadata):
    document_type: str = "deal"
    deal_type: str  # (e.g., "oil_lease", "supply_contract")
    effective_date: datetime
    expiration_date: datetime
    parties: List[Dict[str, str]]
    volume: str
    price_per_unit: float

class RecapMetadata(BaseMetadata):
    document_type: str = "recap"
    meeting_date: datetime
    participants: List[str]
    decisions: List[str]
    action_items: List[str]

class ContractCreate(BaseModel):
    """Schema for creating a new contract."""
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    title: str
    metadata: ContractMetadata
    clauses: List[ClauseBase]

class EmailCreate(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    metadata: EmailMetadata
    content: str
    attachments: List[Dict] = []

class DealCreate(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    metadata: DealMetadata
    clauses: List[ClauseBase]
    financial_terms: Dict[str, Any]

class RecapCreate(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    metadata: RecapMetadata
    summary: str
    key_points: List[str]

class ContractResponse(BaseModel):
    """Response schema for contract operations."""
    id: str
    title: str
    metadata: ContractMetadata
    created_at: datetime
    updated_at: datetime
    clauses_count: int
    status: str = "indexed"

class EmailResponse(BaseModel):
    """Response schema for email operations."""
    id: str
    title: str  # Using subject as title
    metadata: EmailMetadata
    created_at: datetime
    updated_at: datetime
    has_attachments: bool
    status: str = "indexed"

class DealResponse(BaseModel):
    """Response schema for deal operations."""
    id: str
    title: str
    metadata: DealMetadata
    created_at: datetime
    updated_at: datetime
    clauses_count: int
    deal_type: str
    status: str = "indexed"

class RecapResponse(BaseModel):
    """Response schema for recap operations."""
    id: str
    title: str
    metadata: RecapMetadata
    created_at: datetime
    updated_at: datetime
    participants: List[str]
    status: str = "indexed"

class QueryRequest(BaseModel):
    """Schema for search queries."""
    query: str
    filters: Optional[Dict[str, Any]] = None
    top_k: Optional[int] = 5

class SearchResultBase(BaseModel):
    """Base result schema for search."""
    document_id: str
    document_title: str
    content: str
    relevance_score: float
    metadata: Optional[Dict[str, Any]] = None

class ClauseSearchResult(SearchResultBase):
    """Result schema for clause search."""
    clause_id: str
    contract_id: str = Field(alias="document_id")
    contract_title: str = Field(alias="document_title")
    clause_type: str
    clause_title: Optional[str] = None

class EmailSearchResult(SearchResultBase):
    """Result schema for email search."""
    email_id: str = Field(alias="document_id")
    email_subject: str = Field(alias="document_title")
    from_address: Optional[str] = None
    to_addresses: Optional[List[str]] = None

class DealSearchResult(SearchResultBase):
    """Result schema for deal search."""
    deal_id: str = Field(alias="document_id")
    deal_title: str = Field(alias="document_title")
    deal_type: Optional[str] = None
    price_per_unit: Optional[float] = None

class RecapSearchResult(SearchResultBase):
    """Result schema for recap search."""
    recap_id: str = Field(alias="document_id") 
    recap_title: str = Field(alias="document_title")
    meeting_date: Optional[datetime] = None
    participants: Optional[List[str]] = None

class QueryResponse(BaseModel):
    """Response schema for search queries."""
    query: str
    total_results: int
    results: List[Union[ClauseSearchResult, EmailSearchResult, DealSearchResult, RecapSearchResult]]
    metadata: Optional[Dict[str, Any]] = None
    execution_time_ms: float

=== End of ./schemas/schemas.py ===


=== File: ./test.py ===
import pytest
import asyncio
from unittest.mock import MagicMock, patch

from clm_system.core.queryEngine.search import QueryRouter

@pytest.fixture
def sample_contract():
    """Sample contract data for testing."""
    return {
        "id": "test-contract-123",
        "title": "Software License Agreement",
        "metadata": {
            "contract_type": "license",
            "effective_date": "2023-01-01T00:00:00Z",
            "expiration_date": "2024-01-01T00:00:00Z",
            "parties": [
                {"name": "ACME Corp", "id": "party-001"},
                {"name": "Supplier Inc", "id": "party-002"}
            ],
            "status": "active",
            "tags": ["software", "license", "annual"]
        },
        "clauses": [
            {
                "id": "clause-001",
                "title": "License Grant",
                "type": "grant",
                "text": "Licensor hereby grants to Licensee a non-exclusive, non-transferable license to use the Software.",
                "position": 1
            },
            {
                "id": "clause-002",
                "title": "Term and Termination",
                "type": "term",
                "text": "This Agreement shall commence on the Effective Date and continue for a period of one (1) year.",
                "position": 2
            }
        ]
    }

@pytest.mark.asyncio
async def test_query_router_classification():
    """Test query classification logic."""
    router = QueryRouter()
    
    # Test structured query classification
    structured_queries = [
        "contracts with effective date after 2023-01-01",
        "status: active type: license",
        "find contracts with party: ACME Corp",
        "contract type: license"
    ]
    
    for query in structured_queries:
        assert router._classify_query(query) in ["structured", "hybrid"], f"Failed for: {query}"
    
    # Test semantic query classification
    semantic_queries = [
        "what are the license terms",
        "termination conditions",
        "find software agreements"
    ]
    
    for query in semantic_queries:
        assert router._classify_query(query) == "semantic", f"Failed for: {query}"

@pytest.mark.asyncio
async def test_route_query():
    """Test query routing to appropriate search engines."""
    # Mock the Elasticsearch and Qdrant clients
    with patch('clm_system.core.search.ElasticsearchClient') as mock_es, \
         patch('clm_system.core.search.QdrantClient') as mock_qdrant, \
         patch('clm_system.core.search.compute_embedding') as mock_compute_embedding:
        
        # Setup mocks
        mock_es_instance = mock_es.return_value
        mock_es_instance.search.return_value = [{"clause_id": "test1"}]
        
        mock_qdrant_instance = mock_qdrant.return_value
        mock_qdrant_instance.search.return_value = [{"clause_id": "test2"}]
        
        mock_compute_embedding.return_value = [0.1] * 384  # Mock embedding
        
        router = QueryRouter()
        router.es_client = mock_es_instance
        router.qdrant_client = mock_qdrant_instance
        
        # Test structured query routing
        structured_result = await router.route_query("contract type: license", {"status": "active"})
        assert structured_result["query_type"] == "structured"
        assert mock_es_instance.search.called
        
        # Test semantic query routing
        semantic_result = await router.route_query("what are termination conditions")
        assert semantic_result["query_type"] == "semantic"
        assert mock_qdrant_instance.search.called
        
        # Test hybrid query routing
        mock_es_instance.search.reset_mock()
        mock_qdrant_instance.search.reset_mock()
        
        hybrid_result = await router.route_query("find active license contracts with termination clause")
        assert hybrid_result["query_type"] == "hybrid"
        assert mock_es_instance.search.called
        assert mock_qdrant_instance.search.called

=== End of ./test.py ===


=== File: ./test_email_workflow.py ===
import pytest
import asyncio
import logging
from typing import Dict, Any
from datetime import datetime
from clm_system.core.database.mongodb_client import MongoDBClient
from clm_system.core.database.qdrant_client import QdrantClient
from clm_system.core.pipeline.orchestrator import PipelineService
from clm_system.core.utils.embeddings import compute_embedding
from qdrant_client import models

# Configure logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

@pytest.fixture
def sample_email():
    return {
        "id": "email_123",
        "title": "Updated Oil Shipment Schedule",
        "metadata": {
            "document_type": "email",
            "from_address": "logistics@oilcorp.com",
            "to": ["procurement@client.com", "manager@client.com"],
            "cc": ["archive@oilcorp.com"],
            "subject": "Revised Crude Oil Delivery Schedule",
            "sent_date": datetime(2023, 3, 15, 14, 30),
            "tags": ["urgent", "shipment"]
        },
        "attachments": [  # Add this key
            {"filename": "bol_revised.pdf"},
            {"filename": "customs_docs.zip"}
        ],
        "clauses": [
            {
                "id": "body",
                "type": "email_body",
                "text": """Dear Procurement Team,
                
We're updating the March crude oil shipment schedule:
- Tanker OC_Voyager delayed until March 25th
- Increased volume on OC_Explorer to 500,000 barrels
- New pricing at $78/barrel for Brent crude

Attached find the revised Bill of Lading and customs documentation.

Best regards,
OilCorp Logistics Team""",
                "position": 0,
                "metadata": {
                    "attachment_names": ["bol_revised.pdf", "customs_docs.zip"]
                }
            }
        ]
    }
@pytest.mark.asyncio
async def test_email_workflow(sample_email):
    """End-to-end test of email processing and search"""
    
    # Initialize clients
    mongo = MongoDBClient()
    qdrant = QdrantClient()
    pipeline = PipelineService()

    # --- Phase 1: Ingestion ---
    # Process email through pipeline
    processed_email = await pipeline.process_document(sample_email)
    
    # Verify MongoDB insertion
    db_email = await mongo.get_document("email_123")
    assert db_email is not None
    assert db_email["metadata"]["document_type"] == "email"
    assert db_email["metadata"]["subject"] == "Revised Crude Oil Delivery Schedule"
    assert len(db_email["clauses"]) == 1
    
    # --- Phase 2: Embedding Storage ---
    # Verify points are stored in Qdrant
    stored_points = await qdrant.scroll("email_123")
    assert len(stored_points) > 0, "No points stored in Qdrant for email_123"
    logger.info(f"Found {len(stored_points)} points in Qdrant for email_123")
    
    # Debug point structure
    #debug_info = await qdrant.debug_points("email_123")
    #logger.debug(f"Point details:\n{debug_info}")
    
    
    # After verifying points are stored
    logger.info(f"Found {len(stored_points)} points in Qdrant for email_123")

    # Add debugging output
    for point in stored_points:
        logger.info(f"Document ID: {point['document_id']}")
        logger.info(f"Content: {point['content'][:100]}...")
        logger.info(f"Metadata: {point['metadata']}")
    
    # --- Phase 3: Search Verification ---
    test_query = "Current oil shipment prices and schedules"
    query_embedding = compute_embedding(test_query)

    # Search across all document types
    search_results = await qdrant.search(
        embedding=query_embedding,
        filters={
            "document_type": "email",  # Top-level field
            "metadata.has_attachments": True  # Nested under metadata
        },
        top_k=3
    )

    # If still no results, try broader search
    if not search_results:
        backup_results = await qdrant.search(
            embedding=query_embedding,
            filters={"document_type": "email"},
            top_k=3
        )
        logger.info(f"Backup search results: {backup_results}")

    assert len(search_results) > 0, "No search results returned from Qdrant"
    
    # Verify email-specific metadata
    email_result = next(
        (r for r in search_results if r["document_id"] == "email_123"),
        None
    )
    
    assert email_result is not None, "Email result not found in search results"
    assert email_result["document_type"] == "email"
    assert "oil shipment" in email_result["content"].lower()
    assert email_result["metadata"].get("attachment_names") == ["bol_revised.pdf", "customs_docs.zip"]
    
    # --- Phase 4: Temporal Filtering ---
    # Date range search
    date_filtered = await mongo.get_documents(
        filters={
            "metadata.document_type": "email",
            "metadata.sent_date": {
                "$gte": datetime(2023, 3, 1),
                "$lte": datetime(2023, 3, 31)
            }
        },
        sort=[("metadata.sent_date", -1)]
    )
    
    assert len(date_filtered) >= 1
    assert date_filtered[0]["id"] == "email_123"
    
    # --- Phase 5: Cleanup ---
    await mongo.documents_collection.delete_one({"id": "email_123"})
    # qdrant.client.delete(
    #     collection_name="document_chunks",
    #     points_selector=models.FilterSelector(
    #         filter=models.Filter(
    #             must=[
    #                 models.FieldCondition(
    #                     key="document_id",
    #                     match=models.MatchValue(value="email_123")
    #                 )
    #             ]
    #         )
    #     )
    # )

=== End of ./test_email_workflow.py ===


=== File: ./__init__.py ===


=== End of ./__init__.py ===


